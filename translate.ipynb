{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swyoo5/LlamaFineTuningProject/blob/main/translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNHlnnVlEDrg",
        "outputId": "e7d350ac-8075-4093-8da4-8d7e27dde3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU :  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU : \", len(tf.config.experimental.list_physical_devices(\"GPU\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MoclIjZK1Zm",
        "outputId": "6204e8af-d024-4718-a111-ff7aa7d61f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May  9 06:36:06 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8              11W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# ì—”ë¹„ë””ì•„ gpu ë“œë¼ì´ë²„ì™€ ê´€ë ¨ëœ ì‹œìŠ¤í…œ ë° í•˜ë“œì›¨ì–´ ì •ë³´ë¥¼ ì¡°íšŒ\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m6CkJRBK7XY",
        "outputId": "09cae48e-fb76-4539-b99b-233fd6529fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/213.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m204.8/213.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m174.1/174.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q autotrain-advanced==0.6.80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_Lqt8wJK-xr",
        "outputId": "fb07e31d-5c98-4618-e67f-2f4c91801562"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "> \u001b[1mINFO    Installing latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest PyTorch\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest PyTorch\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!autotrain setup --update-torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTONUBYGe7NW"
      },
      "source": [
        "# íŒŒì¸íŠœë‹í•˜ê¸°\n",
        "llm --train : trainì„ ìˆ˜í–‰í•˜ëŠ” llm ì§€ì •\n",
        "\n",
        "project-name : í•™ìŠµê³¼ì •ì´ ì €ì¥ë  í´ë”ëª…\n",
        "\n",
        "data-path : íŒŒì¸íŠœë‹ì— ì‚¬ìš©ë  ë°ì´í„°ì˜ ê²½ë¡œ, í•´ë‹¹íŒŒì¼ ì•ˆì— jsoníŒŒì¼ì´ ìˆì–´ì•¼ í•¨(conversational1.json).\n",
        "\n",
        "text-column : í•™ìŠµë°ì´í„°ì—ì„œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ì—´ ì§€ì •\n",
        "\n",
        "peft : peft(parameter efficient fine tuning) í™œì„±í™”\n",
        "\n",
        "quantization : ì–‘ìí™” ë°©ë²•ì„ ì§€ì •(int4 : 4ë¹„íŠ¸ ì •ìˆ˜ ì–‘ìí™”)\n",
        "\n",
        "lr : í•™ìŠµë¥  ì§€ì •\n",
        "\n",
        "batch-size : ë°°ì¹˜ í¬ê¸° ì„¤ì •. í•œë²ˆì— ì²˜ë¦¬í•  ìƒ˜í”Œì˜ ìˆ˜\n",
        "\n",
        "epochs : ì—í­ìˆ˜\n",
        "\n",
        "trainer : í•™ìŠµì— ì‚¬ìš©í•  íŠ¸ë ˆì´ë„ˆ(sft : supervised fine tuning)\n",
        "\n",
        "model_max_length : ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì…ë ¥ ê¸¸ì´ ì§€ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2z966NSLCVh",
        "outputId": "ac5d8ffd-0e07-4d49-c672-aeb4c1662c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "> \u001b[1mINFO    Running LLM\u001b[0m\n",
            "> \u001b[1mINFO    Params: Namespace(version=False, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model_ref=None, warmup_ratio=0.1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.0, max_grad_norm=1.0, add_eos_token=False, block_size=-1, peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, mixed_precision=None, quantization='int4', model_max_length=80, trainer='sft', target_modules=None, merge_adapter=False, use_flash_attention_2=False, dpo_beta=0.1, apply_chat_template=False, padding=None, train=True, deploy=False, inference=False, username=None, backend='local-cli', token=None, repo_id=None, push_to_hub=False, model='TinyPixel/Llama-2-7B-bf16-sharded', project_name='conversational-finetuning', seed=42, epochs=10, gradient_accumulation=1, disable_gradient_checkpointing=False, lr=0.0003, log='none', data_path='conversational-prompt', train_split='train', valid_split=None, batch_size=8, func=<function run_llm_command_factory at 0x7d73d2df6170>)\u001b[0m\n",
            "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
            "> \u001b[1mINFO    {\"model\":\"TinyPixel/Llama-2-7B-bf16-sharded\",\"project_name\":\"conversational-finetuning\",\"data_path\":\"conversational-prompt\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":-1,\"model_max_length\":80,\"padding\":null,\"trainer\":\"sft\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":false,\"mixed_precision\":null,\"lr\":0.0003,\"epochs\":10,\"batch_size\":8,\"warmup_ratio\":0.1,\"gradient_accumulation\":1,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.0,\"max_grad_norm\":1.0,\"seed\":42,\"apply_chat_template\":false,\"quantization\":\"int4\",\"target_modules\":null,\"merge_adapter\":false,\"peft\":true,\"lora_r\":16,\"lora_alpha\":32,\"lora_dropout\":0.05,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"prompt\",\"text_column\":\"text\",\"rejected_text_column\":\"rejected\",\"push_to_hub\":false,\"repo_id\":null,\"username\":null,\"token\":null}\u001b[0m\n",
            "> \u001b[1mINFO    ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'conversational-finetuning/training_params.json']\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "\u001b[1mğŸš€ INFO  \u001b[0m | \u001b[32m2024-05-09 07:18:34\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mTrain data: Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 200\n",
            "})\u001b[0m\n",
            "\u001b[1mğŸš€ INFO  \u001b[0m | \u001b[32m2024-05-09 07:18:34\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
            "Loading checkpoint shards: 100% 14/14 [01:07<00:00,  4.83s/it]\n",
            "\u001b[1mğŸš€ INFO  \u001b[0m | \u001b[32m2024-05-09 07:19:43\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mUsing block size 80\u001b[0m\n",
            "\u001b[1mğŸš€ INFO  \u001b[0m | \u001b[32m2024-05-09 07:19:43\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m339\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "  0% 0/250 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (114 > 80). Running this sequence through the model will result in indexing errors\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 2.8454, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.2}\n",
            "{'loss': 2.8365, 'learning_rate': 0.00011999999999999999, 'epoch': 0.4}\n",
            "{'loss': 2.466, 'learning_rate': 0.00017999999999999998, 'epoch': 0.6}\n",
            "  6% 16/250 [02:12<32:14,  8.27s/it]Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "Process ForkProcess-2:\n",
            "Process ForkProcess-1:\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/autotrain\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/cli/autotrain.py\", line 48, in main\n",
            "    command.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/cli/run_llm.py\", line 383, in run\n",
            "    _ = project.create()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/project.py\", line 71, in create\n",
            "    return self.create_spaces()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/project.py\", line 64, in create_spaces\n",
            "    space_id = sr.prepare()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/backend.py\", line 103, in prepare\n",
            "    space_id = self._create_space()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/backend.py\", line 243, in _create_space\n",
            "    pid = local_runner.create()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/backend.py\", line 287, in create\n",
            "    training_pid = run_training(params, task_id, local=True, wait=self.wait)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/app_utils.py\", line 138, in run_training\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1222, in wait\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1017, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 634, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/__main__.py\", line 528, in <module>\n",
            "    train(_config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/common.py\", line 87, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/__main__.py\", line 475, in train\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 280, in train\n",
            "    output = super().train(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1854, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2737, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1905, in backward\n",
            "    loss.backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 522, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 266, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "  6% 16/250 [02:20<34:12,  8.77s/it]\n"
          ]
        }
      ],
      "source": [
        "!autotrain llm --train \\\n",
        "  --project-name \"conversational-finetuning\" \\\n",
        "  --model \"TinyPixel/Llama-2-7B-bf16-sharded\" \\\n",
        "  --data-path \"conversational-prompt\" \\\n",
        "  --text-column \"text\" \\\n",
        "  --peft \\\n",
        "  --quantization \"int4\" \\\n",
        "  --lr 3e-4 \\\n",
        "  --batch-size 8 \\\n",
        "  --epochs 10 \\\n",
        "  --trainer sft \\\n",
        "  --model_max_length 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eGmPk9zgs0A"
      },
      "source": [
        "# ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvksWm9UV1X6"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "folder_name = \"conversational-finetuning\"\n",
        "zip_file_name = \"conversational-finetuning.zip\"\n",
        "shutil.make_archive(zip_file_name[:-4], \"zip\", folder_name)\n",
        "files.download(zip_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je2WsyGGrGy1"
      },
      "source": [
        "# Llama ë²ˆì—­ì‘ì—…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMHe3-P4rJQV"
      },
      "outputs": [],
      "source": [
        "extract_folder_name = '.' # í˜„ì¬ ë””ë ‰í† ë¦¬\n",
        "\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref :\n",
        "  zip_ref.extractall(extract_folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d7dd2132503a4fc9b0e4c3fa94fecd06",
            "7db01cd277b94160aef754289494575a",
            "91b60ca6055148e8b894fcb66c5b54c0",
            "c8b08b4c049144e295c9af350186fb21",
            "9eccac0dc9e6480cb301259b95369c68",
            "10f91eb2dade4c1d86273e1084e73c9e",
            "f495532cbd084f5f9eb79437b96405a4",
            "c27000ea89f44ac28d7a142dc4a32dde",
            "c2d2832c14b048be9e9da83efadd77f9",
            "2f2257da065149dbab8ffc5304957a12",
            "7b71dded5960402f8f4fa343e1249d65"
          ]
        },
        "id": "BfvoDQPytIJs",
        "outputId": "b99ce0c3-e7e4-430c-a58d-2785095e0718"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7dd2132503a4fc9b0e4c3fa94fecd06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "model_id = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
        "peft_model_id = \"/content/checkpoint-250\" # peftë¥¼ ì ìš©í•  ì²´í¬í¬ì¸íŠ¸\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id) # peftë¥¼ ìœ„í•œ ì„¤ì •ì„ ë¯¸ë¦¬ í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œí•œë‹¤.\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit = False, # 8ë¹„íŠ¸ ì •ìˆ˜í˜•ìœ¼ë¡œ ë¡œë“œí• ì§€ ì—¬ë¶€\n",
        "    load_in_4bit = True, # 4ë¹„íŠ¸ ì •ìˆ˜í˜•ìœ¼ë¡œ ë¡œë“œí• ì§€ ì—¬ë¶€\n",
        "    llm_int8_threshold = 6.0, # llmì˜ 8ë¹„íŠ¸ ì •ìˆ˜í™”ë¥¼ ìœ„í•œ ì„ê³„ê°’\n",
        "    llm_int8_enable_fp32_cpu_offload = False, # llm 8ë¹„íŠ¸ ì •ìˆ˜í™”ë¥¼ ìœ„í•´ cpu offload í™œì„±í™” ì—¬ë¶€\n",
        "    llm_int8_has_fp16_weight = False, # 8ë¹„íŠ¸ ì •ìˆ˜í™”ì— fp16 ê°€ì¤‘ì¹˜ê°€ ìˆëŠ”ì§€ ì—¬ë¶€ ì„¤ì •\n",
        "    bnb_4bit_quant_type = \"nf4\", # 4ë¹„íŠ¸ ì–‘ìí™” ìœ í˜•\n",
        "    bnb_4bit_use_double_quant = False, # ì´ì¤‘ ì–‘ìí™”ë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€\n",
        "    bnb_4bit_compute_dtype = \"float16\" # 4ë¹„íŠ¸ ì—°ì‚°ì— ì‚¬ìš©í•  ë°ì´í„° ìœ í˜•\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config = bnb_config, device_map = {\"\":0}) # ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œ, bits and bytes ì•Œê³ ë¦¬ì¦˜ì˜ ì„¤ì •ì„ ì ìš©\n",
        "model = PeftModel.from_pretrained(model, peft_model_id) # ì´ì „ì— ë¡œë“œí•œ llmëª¨ë¸ì— peft ì ìš©\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id) # í† í¬ë‚˜ì´ì € ë¡œë“œ, ì‚¬ì „í•™ìŠµ ëª¨ë¸ì´ ì‚¬ìš©í•œ í† í¬ë‚˜ì´ì €\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mh_SOSyuV49"
      },
      "outputs": [],
      "source": [
        "prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\\n",
        "### Instruction : Korean and English sentences are given in order below. Translate Korean sentences into English sentences.\\\n",
        "### Korean : %s ### English : \"\n",
        "\n",
        "def gen(x) :\n",
        "  q = prompt % (x,)\n",
        "  gened = model.generate(\n",
        "      **tokenizer(\n",
        "          q,\n",
        "          return_tensors = \"pt\",\n",
        "          return_token_type_ids = False\n",
        "      ).to(\"cuda\"),\n",
        "      max_new_tokens = 128,\n",
        "      early_stopping = True,\n",
        "      do_sample = False\n",
        "  )\n",
        "  return tokenizer.decode(gened[0]).replace(q, '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n46rFmyWhQqZ"
      },
      "source": [
        "# Translate\n",
        "3ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤.\n",
        "\n",
        "1. í›ˆë ¨ë°ì´í„° ì¤‘ ì„ì˜ë¡œ ì„ íƒí•´ augmentëœ ë°ì´í„° + í›ˆë ¨ ë°ì´í„°ì— í¬í•¨ë˜ì§€ ì•Šì€ ë°ì´í„° ì¤‘ ì„ì˜ë¡œ ì„ íƒ\n",
        "\n",
        "  => ì¶œë ¥ ë¬¸ì¥ê³¼ ì •ë‹µ ë¬¸ì¥ ê°„ì˜ bleu score ì¸¡ì •\n",
        "\n",
        "2. ChatGPTê°€ ì„ì˜ë¡œ ìƒì„±í•œ ë¬¸ì¥\n",
        "  \n",
        "  => êµ¬ê¸€ ë²ˆì—­ê¸°ì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ ì •ë‹µ ë¬¸ì¥ìœ¼ë¡œ í•´ì„œ bleu score ì¸¡ì •\n",
        "\n",
        "3. í›ˆë ¨ ë°ì´í„°ì—ì„œ ì„ì˜ë¡œ ë¬¸ì¥ì„ ì„ íƒ\n",
        "  \n",
        "  => êµ¬ê¸€ ë²ˆì—­ê¸°ì˜ scoreì™€ Llamaì˜ score ë¹„êµ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ë°©ë²•1"
      ],
      "metadata": {
        "id": "46iju_CSNELL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "í›ˆë ¨ ë°ì´í„°ì—ì„œ ì„ì˜ë¡œ ì„ íƒ í›„ augmentí•œ ë¬¸ì¥"
      ],
      "metadata": {
        "id": "3skfDHSs9pwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translate = [] # ëª¨ë¸ ë²ˆì—­ë¬¸ì¥\n",
        "correct = [] # ì •ë‹µ ë¬¸ì¥"
      ],
      "metadata": {
        "id": "zZkHnvBdjZaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e490FjEhSix",
        "outputId": "16bbc7ed-6309-4e7e-f820-f3bf99a53124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:430: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> 2 years ago, we started to work the same way as we are working now.</s>\n"
          ]
        }
      ],
      "source": [
        "# ì›ë˜ë¬¸ì¥ : ë¹„ìš©ì´ ë§ì´ ë“¤ì–´ì„œ ì§€ê¸ˆê³¼ ê°™ì€ ë°©ë²•ì„ íƒí–ˆìŠµë‹ˆë‹¤.\n",
        "# ì •ë‹µ : We chose the current way due to the cost.\n",
        "print(gen(\"ë¹„ìš© ë¬¸ì œë¡œ ì¸í•´ ì§€ê¸ˆì²˜ëŸ¼ ì ‘ê·¼í–ˆìŠµë‹ˆë‹¤.\"))\n",
        "translate.append(gen(\"ë¹„ìš© ë¬¸ì œë¡œ ì¸í•´ ì§€ê¸ˆì²˜ëŸ¼ ì ‘ê·¼í–ˆìŠµë‹ˆë‹¤.\"))\n",
        "correct.append(\"We chose the current way due to the cost.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2j2iXdnhy4y",
        "outputId": "fe6fd041-6896-4d48-f79b-6a2e2428bba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Below the email is raised with more specific details in smaller letters.</s>\n"
          ]
        }
      ],
      "source": [
        "# ì›ë˜ë¬¸ì¥ : êµìœ¡ ì•ˆë‚´ ë©”ì¼ í•˜ë‹¨ì— ì‘ì€ ê¸€ì”¨ë¡œ ì í˜€ìˆì–´ìš”.\n",
        "# ì •ë‹µ : It was written in very small letters at the bottom of the notification email.\n",
        "print(gen(\"êµìœ¡ ì•ˆë‚´ ì´ë©”ì¼ì˜ ì•„ë˜ì—ëŠ” ì‘ì€ ê¸€ìë¡œ ë” ë§ì€ ì„¸ë¶€ ì‚¬í•­ì´ ê¸°ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\"))\n",
        "translate.append(gen(\"êµìœ¡ ì•ˆë‚´ ì´ë©”ì¼ì˜ ì•„ë˜ì—ëŠ” ì‘ì€ ê¸€ìë¡œ ë” ë§ì€ ì„¸ë¶€ ì‚¬í•­ì´ ê¸°ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\"))\n",
        "correct.append(\"It was written in very small letters at the bottom of the notification email.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJXOnuuGillG",
        "outputId": "b4f2656e-8cd1-4a0f-bc81-6161600e91cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  I'll ask them to leave that place, so that we can store the new products there.</s>\n"
          ]
        }
      ],
      "source": [
        "# ì›ë˜ë¬¸ì¥ : ë„¤, ê·¸ìª½ì— ì—°ë½í•´ì„œ ì‹ ì œí’ˆ ë³´ê´€ì„ ìœ„í•´ ë¹„ì›Œë‘ë„ë¡ í• ê²Œìš”.\n",
        "# ì •ë‹µ : Sure, I'll contact them and clear the warehouse for it.\n",
        "print(gen(\"ê·¸ê³³ì— ì—°ë½í•´ì„œ ì‹ ì œí’ˆ ë³´ê´€ì„ ìœ„í•´ ê·¸ ê³³ì„ ë¹„ì›Œë‘ë„ë¡ ì¡°ì¹˜í•˜ê² ìŠµë‹ˆë‹¤.\"))\n",
        "translate.append(gen(\"ê·¸ê³³ì— ì—°ë½í•´ì„œ ì‹ ì œí’ˆ ë³´ê´€ì„ ìœ„í•´ ê·¸ ê³³ì„ ë¹„ì›Œë‘ë„ë¡ ì¡°ì¹˜í•˜ê² ìŠµë‹ˆë‹¤.\"))\n",
        "correct.append(\"Sure, I'll contact them and clear the warehouse for it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQXen-_2jb_j",
        "outputId": "6e411876-d9cd-4029-ef6e-07e0a128a23c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Where is the biggest warehouse of our company located?</s>\n"
          ]
        }
      ],
      "source": [
        "# ì›ë˜ë¬¸ì¥ : íšŒì‚¬ ì°½ê³  ì¤‘ì— ê°€ì¥ í° ì°½ê³ ëŠ” ì–´ëŠ ì§€ì—­ì˜ ì°½ê³ ì¸ê°€ìš”?\n",
        "# ì •ë‹µ : Where is the biggest warehouse in our company located?\n",
        "print(gen(\"íšŒì‚¬ì˜ ê°€ì¥ í° ì°½ê³ ëŠ” ì–´ëŠ ë„ì‹œì— ìœ„ì¹˜í•˜ê³  ìˆë‚˜ìš”?\"))\n",
        "translate.append(gen(\"íšŒì‚¬ì˜ ê°€ì¥ í° ì°½ê³ ëŠ” ì–´ëŠ ë„ì‹œì— ìœ„ì¹˜í•˜ê³  ìˆë‚˜ìš”?\"))\n",
        "correct.append(\"Where is the biggest warehouse in our company located?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9-sKYbsjtz9",
        "outputId": "e777971e-57fb-47af-a8ba-5f1433225f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Since the number of people increased, we need a bigger room.</s>\n"
          ]
        }
      ],
      "source": [
        "# ì›ë˜ë¬¸ì¥ : ë„¤, ì°¸ì„ì˜ˆìƒì¸ì›ì´ ë§ì´ ëŠ˜ì–´ì„œ ë„“ì€ ê³µê°„ì´ í•„ìš”í•´ìš”.\n",
        "# ì •ë‹µ : Yes, there will be more people than we expected, so we need a bigger room.\n",
        "print(gen(\"ì°¸ì„ ì˜ˆì • ì¸ì›ì´ ëŠ˜ì–´ë‚˜ì„œ ë” ë„“ì€ ê³µê°„ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.\"))\n",
        "translate.append(gen(\"ì°¸ì„ ì˜ˆì • ì¸ì›ì´ ëŠ˜ì–´ë‚˜ì„œ ë” ë„“ì€ ê³µê°„ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.\"))\n",
        "correct.append(\"Yes, there will be more people than we expected, so we need a bigger room.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJmkzYWskKIR",
        "outputId": "0f6c367c-b59b-45bb-afff-2321e523a487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Would it be possible to leave today or tomorrow?</s>\n"
          ]
        }
      ],
      "source": [
        "# ì›ë˜ë¬¸ì¥ : í˜¹ì‹œ ì˜¤ëŠ˜ì´ë‚˜ ë‚´ì¼ ë°”ë¡œ ì¶œì¥ì„ ë– ë‚  ìˆ˜ ìˆë‚˜ìš”?\n",
        "# ì •ë‹µ : Can you go on a business trip today or tomorrow?\n",
        "print(gen(\"ì˜¤ëŠ˜ì´ë‚˜ ë‚´ì¼ ì¤‘ì— ì¶œì¥ì„ ë– ë‚˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•œê°€ìš”?\"))\n",
        "translate.append(gen(\"ì˜¤ëŠ˜ì´ë‚˜ ë‚´ì¼ ì¤‘ì— ì¶œì¥ì„ ë– ë‚˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•œê°€ìš”?\"))\n",
        "correct.append(\"Can you go on a business trip today or tomorrow?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8evebVTklsL",
        "outputId": "75c553b8-9e83-44ca-e6ee-3fb4f84e075d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> 2 times already, the photocopier broke down, so I handed it to the maintenance.</s>\n"
          ]
        }
      ],
      "source": [
        "# ì›ë˜ë¬¸ì¥ : ë³µì‚¬ê¸°ê°€ ë˜ ê³ ì¥ ë‚˜ì„œ ì‹œì„¤ ê´€ë¦¬íŒ€ì— ì—°ë½í–ˆì–´ìš”.\n",
        "# ì •ë‹µ : The copy machine broke down again, so I called the maintenance.\n",
        "print(gen(\"ë˜ ë‹¤ì‹œ ë³µì‚¬ê¸°ê°€ ê³ ì¥ ë‚¬ìœ¼ë‹ˆ ì‹œì„¤ ê´€ë¦¬íŒ€ì— ì ‘ìˆ˜í–ˆìŠµë‹ˆë‹¤.\"))\n",
        "translate.append(gen(\"ë˜ ë‹¤ì‹œ ë³µì‚¬ê¸°ê°€ ê³ ì¥ ë‚¬ìœ¼ë‹ˆ ì‹œì„¤ ê´€ë¦¬íŒ€ì— ì ‘ìˆ˜í–ˆìŠµë‹ˆë‹¤.\"))\n",
        "correct.append(\"The copy machine broke down again, so I called the maintenance.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skDI0lamk73Y",
        "outputId": "e55285f1-af51-4ab9-cd2c-03ac17df8fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Would there be any other solutions besides adjusting the price?</s>\n"
          ]
        }
      ],
      "source": [
        "# ì›ë˜ë¬¸ì¥ : ê·¸ëŸ¼ ì œí’ˆ ê°€ê²©ì„ ë‚®ì¶”ëŠ” ê²ƒ ì™¸ì— ë‹¤ë¥¸ ë°©ë²•ì´ ìˆë‚˜ìš”?\n",
        "# ì •ë‹µ : Then, is there any other way except lowering the price?\n",
        "print(gen(\"ì œí’ˆ ê°€ê²©ì„ ì¡°ì •í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ë‹¤ë¥¸ í•´ê²°ì±…ì´ ìˆì„ê¹Œìš”?\"))\n",
        "translate.append(gen(\"ì œí’ˆ ê°€ê²©ì„ ì¡°ì •í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ë‹¤ë¥¸ í•´ê²°ì±…ì´ ìˆì„ê¹Œìš”?\"))\n",
        "correct.append(\"Then, is there any other way except lowering the price?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtQfrZiFlTjQ",
        "outputId": "e7957a79-296e-433a-d0fe-af1796e67408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  sales are steadily increasing, so I'll consider taking the big order.</s>\n"
          ]
        }
      ],
      "source": [
        "# ì›ë˜ë¬¸ì¥ : ê¾¸ì¤€íˆ ë§¤ì¶œì´ ì¦ê°€í•˜ê³  ìˆëŠ”ë° í•œ ë²ˆì— ì£¼ë¬¸í•˜ëŠ” ê²Œ ë‚«ì£ .\n",
        "# ì •ë‹µ : The sales are consistently increasing, so it's better to order them at once.\n",
        "print(gen(\"ë§¤ì¶œì´ ê¾¸ì¤€íˆ ëŠ˜ì–´ë‚˜ê³  ìˆìœ¼ë‹ˆ ëŒ€ê·œëª¨ ì£¼ë¬¸ì„ ê³ ë ¤í•´ ë³´ëŠ” ê²ƒì´ ì¢‹ê² ì–´ìš”.\"))\n",
        "translate.append(gen(\"ë§¤ì¶œì´ ê¾¸ì¤€íˆ ëŠ˜ì–´ë‚˜ê³  ìˆìœ¼ë‹ˆ ëŒ€ê·œëª¨ ì£¼ë¬¸ì„ ê³ ë ¤í•´ ë³´ëŠ” ê²ƒì´ ì¢‹ê² ì–´ìš”.\"))\n",
        "correct.append(\"The sales are consistently increasing, so it's better to order them at once.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train ë°ì´í„°ì— í¬í•¨ë˜ì§€ ì•Šì€ ë¬¸ì¥ì„ ì…ë ¥"
      ],
      "metadata": {
        "id": "rHeem61XNUOC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgHTqTvCXd1v",
        "outputId": "f3b97a31-7e69-45d1-da43-264828ad72bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Great, but the new employees may have some problems.</s>\n"
          ]
        }
      ],
      "source": [
        "# ì •ë‹µ : Sounds good, the new employees seem to have a hard time getting used to it.\n",
        "print(gen(\"ì¢‹ì•„ìš”, ì•ˆ ê·¸ë˜ë„ ì‹ ì… ì§ì›ë“¤ì€ ì¡°ê¸ˆ ì–´ë ¤ì›Œí•˜ë”ë¼ê³ ìš”.\"))\n",
        "translate.append(gen(\"ì¢‹ì•„ìš”, ì•ˆ ê·¸ë˜ë„ ì‹ ì… ì§ì›ë“¤ì€ ì¡°ê¸ˆ ì–´ë ¤ì›Œí•˜ë”ë¼ê³ ìš”.\"))\n",
        "correct.append(\"Sounds good, the new employees seem to have a hard time getting used to it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyJnu6MDGmq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7555007-45a6-4ac2-9601-0d840ced4387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Last semester, I had to take an intern, but it all went wrong.</s>\n"
          ]
        }
      ],
      "source": [
        "# ì •ë‹µ : I had to do an intern last semester, but I totally failed.\n",
        "print(gen(\"ì €ë²ˆ í•™ê¸°ì— ì¸í„´ì„ í•´ì•¼ í–ˆëŠ”ë°, ë‹¤ ë–¨ì–´ì¡Œì–´.\"))\n",
        "translate.append(gen(\"ì €ë²ˆ í•™ê¸°ì— ì¸í„´ì„ í•´ì•¼ í–ˆëŠ”ë°, ë‹¤ ë–¨ì–´ì¡Œì–´.\"))\n",
        "correct.append(\"I had to do an intern last semester, but I totally failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpSaCwfdG4gI",
        "outputId": "0b20c5fb-e5c3-4840-a238-d8dd2d7022f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  I have a feeling that I made the time table too rigid.</s>\n"
          ]
        }
      ],
      "source": [
        "# ì •ë‹µ : I think you overworked your timetable.\n",
        "print(gen(\"ë„¤ê°€ ì‹œê°„í‘œë¥¼ ë„ˆë¬´ ë¬´ë¦¬í•˜ê²Œ ì§  ê²ƒ ê°™ì€ ìƒê°ì´ ë“ ë‹¤.\"))\n",
        "translate.append(gen(\"ë„¤ê°€ ì‹œê°„í‘œë¥¼ ë„ˆë¬´ ë¬´ë¦¬í•˜ê²Œ ì§  ê²ƒ ê°™ì€ ìƒê°ì´ ë“ ë‹¤.\"))\n",
        "correct.append(\"I think you overworked your timetable.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chqxT3I1HZJJ",
        "outputId": "366eb27f-9d55-4ecb-ecc3-c4d051045264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Why did you register Chinese if you can't speak Chinese?</s>\n"
          ]
        }
      ],
      "source": [
        "# ì •ë‹µ : Why did you enroll in Chinese class when you can't speak Chinese?\n",
        "print(gen(\"ë„ˆ ì¤‘êµ­ì–´ë„ ëª»í•˜ë©´ì„œ ì™œ ì¤‘êµ­ì–´ ìˆ˜ì—…ì„ ë“±ë¡í–ˆì–´?\"))\n",
        "translate.append(gen(\"ë„ˆ ì¤‘êµ­ì–´ë„ ëª»í•˜ë©´ì„œ ì™œ ì¤‘êµ­ì–´ ìˆ˜ì—…ì„ ë“±ë¡í–ˆì–´?\"))\n",
        "correct.append(\"Why did you enroll in Chinese class when you can't speak Chinese?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU1"
      ],
      "metadata": {
        "id": "R1aUn0hCnJl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translate = [t.replace(\"<s>\", '').replace(\"</s>\", '').strip() for t in translate]"
      ],
      "metadata": {
        "id": "bNs7JzXdoCXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWX85VJupCWE",
        "outputId": "9c61353e-70b8-4f24-fd0b-2ef3514ff955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2 years ago, we started to work the same way as we are working now.',\n",
              " 'Below the email is raised with more specific details in smaller letters.',\n",
              " \"I'll ask them to leave that place, so that we can store the new products there.\",\n",
              " 'Where is the biggest warehouse of our company located?',\n",
              " 'Since the number of people increased, we need a bigger room.',\n",
              " 'Would it be possible to leave today or tomorrow?',\n",
              " '2 times already, the photocopier broke down, so I handed it to the maintenance.',\n",
              " 'Would there be any other solutions besides adjusting the price?',\n",
              " \"sales are steadily increasing, so I'll consider taking the big order.\",\n",
              " 'Great, but the new employees may have some problems.',\n",
              " 'Last semester, I had to take an intern, but it all went wrong.',\n",
              " 'I have a feeling that I made the time table too rigid.',\n",
              " \"Why did you register Chinese if you can't speak Chinese?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmdqGY2apDqa",
        "outputId": "9a24dbf1-40e2-41be-f6c9-576df598d543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We chose the current way due to the cost.',\n",
              " 'It was written in very small letters at the bottom of the notification email.',\n",
              " \"Sure, I'll contact them and clear the warehouse for it.\",\n",
              " 'Where is the biggest warehouse in our company located?',\n",
              " 'Yes, there will be more people than we expected, so we need a bigger room.',\n",
              " 'Can you go on a business trip today or tomorrow?',\n",
              " 'The copy machine broke down again, so I called the maintenance.',\n",
              " 'Then, is there any other way except lowering the price?',\n",
              " \"The sales are consistently increasing, so it's better to order them at once.\",\n",
              " 'Sounds good, the new employees seem to have a hard time getting used to it.',\n",
              " 'I had to do an intern last semester, but I totally failed.',\n",
              " 'I think you overworked your timetable.',\n",
              " \"Why did you enroll in Chinese class when you can't speak Chinese?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "bleu_list = []\n",
        "for i in range(13) :\n",
        "  c = correct[i].split()\n",
        "  t = translate[i].split()\n",
        "\n",
        "  bleu_list.append(sentence_bleu([c], t, weights = (1, 0, 0, 0)))\n",
        "\n",
        "bleu_score = np.mean(bleu_list)\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiLYcNm3nQhD",
        "outputId": "9461805b-7379-424f-d022-79b564f353e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3602527603189402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë°©ë²•2\n",
        "ChatGPTê°€ ìƒì„±í•œ ì„ì˜ì˜ ë¬¸ì¥"
      ],
      "metadata": {
        "id": "gaRliqgvR3Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_sentences = [\n",
        "    \"ì§€ë‚œ ì£¼ë§ì—ëŠ” ê°€ì¡±ê³¼ í•¨ê»˜ í•´ë³€ìœ¼ë¡œ ì—¬í–‰ì„ ë‹¤ë…€ì™”ëŠ”ë°, ì •ë§ë¡œ ì‹œì›í•˜ê³  ì¦ê±°ìš´ ì‹œê°„ì„ ë³´ëƒˆìŠµë‹ˆë‹¤.\",\n",
        "    \"ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ë©´ì„œ ë§ì€ ë„ì „ì— ì§ë©´í–ˆì§€ë§Œ, ì°½ì˜ì ì¸ í•´ê²°ì±…ì„ ì°¾ì•„ë‚´ì–´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.\",\n",
        "    \"ìš”ì¦˜ì—ëŠ” ì±…ì„ ì½ëŠ” ê²ƒì— ë¹ ì ¸ ìˆì–´ì„œ ë§¤ì¼ ë°¤ ì±…ì„ ì½ìœ¼ë©° ìƒˆë¡œìš´ ì§€ì‹ì„ ìŠµë“í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
        "    \"ìµœê·¼ì— ìƒˆë¡œìš´ ì·¨ë¯¸ë¡œ ìš”ë¦¬ë¥¼ ì‹œì‘í–ˆëŠ”ë°, ë‹¤ì–‘í•œ ë ˆì‹œí”¼ë¥¼ ì‹¤í—˜í•˜ë©´ì„œ ë§›ìˆëŠ” ìŒì‹ì„ ë§Œë“¤ê³  ê°€ì¡±ë“¤ê³¼ í•¨ê»˜ ì¦ê¸°ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ì—¬ë¦„ íœ´ê°€ ë•ŒëŠ” ì—¬ëŸ¬ ë‚˜ë¼ë¥¼ ì—¬í–‰í•˜ë©´ì„œ ë‹¤ì–‘í•œ ë¬¸í™”ë¥¼ ê²½í—˜í•˜ê³  ìƒˆë¡œìš´ ì¹œêµ¬ë“¤ì„ ì‚¬ê·€ì—ˆìŠµë‹ˆë‹¤.\",\n",
        "    \"ìµœê·¼ì—ëŠ” ê±´ê°•ì„ ìœ„í•´ ìš´ë™ì„ ê¾¸ì¤€íˆ í•˜ê³  ìˆì–´ì„œ ëª¸ë„ ê±´ê°•í•˜ê³  ê¸°ë¶„ë„ ì¢‹ìŠµë‹ˆë‹¤.\",\n",
        "    \"ë‹¤ê°€ì˜¤ëŠ” í”„ë¡œì íŠ¸ì— ëŒ€í•´ íŒ€ì›ë“¤ê³¼ ì•„ì´ë””ì–´ë¥¼ ê³µìœ í•˜ê³  í˜‘ë ¥í•˜ì—¬ ìµœìƒì˜ ê²°ê³¼ë¥¼ ì´ëŒì–´ ë‚´ê³ ì í•©ë‹ˆë‹¤.\",\n",
        "    \"ì–´ë¦´ ë•Œë¶€í„° ìŒì•…ì— ê´€ì‹¬ì´ ë§ì•„ì„œ í˜¼ì ê¸°íƒ€ë¥¼ ì—°ìŠµí•˜ê³  ë‹¤ì–‘í•œ ìŒì•…ì„ ì¦ê¸°ëŠ” ì·¨ë¯¸ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
        "    \"ì£¼ë§ì—ëŠ” ì‹œê°„ì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ê¸°ìˆ ì„ í•™ìŠµí•˜ê³  ìê¸° ê³„ë°œì— í˜ì“°ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
        "    \"ë‹¤ê°€ì˜¤ëŠ” ê²¨ìš¸ ì‹œì¦Œì—ëŠ” ê°€ì¡±ê³¼ í•¨ê»˜ ìŠ¤í‚¤ ì—¬í–‰ì„ ê³„íš ì¤‘ì´ì–´ì„œ ê¸°ëŒ€ê°€ ë§ì´ ë©ë‹ˆë‹¤.\"\n",
        "]\n",
        "\n",
        "gpt_google_translate = [\n",
        "    \"Last weekend I went on a trip to the beach with my family and had a really cool and fun time.\",\n",
        "    \"We faced many challenges when starting a new project, but we found creative solutions and completed it successfully.\",\n",
        "    \"These days, I am obsessed with reading books, and I am acquiring new knowledge by reading books every night.\",\n",
        "    \"I recently started cooking as a new hobby, experimenting with various recipes to make delicious food and enjoy it with my family.\",\n",
        "    \"During my summer vacation, I traveled to different countries, experienced different cultures and made new friends.\",\n",
        "    \"Recently, I have been exercising regularly for my health, so my body is healthy and I feel good.\",\n",
        "    \"We want to share ideas and collaborate with our team members on upcoming projects to achieve the best results.\",\n",
        "    \"I have been interested in music since I was young, and have hobbies of practicing guitar on my own and enjoying a variety of music.\",\n",
        "    \"On the weekends, I use my time to learn new skills and develop myself.\",\n",
        "    \"I'm planning a ski trip with my family this coming winter season, so I'm really looking forward to it.\"\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5E-tpsLRZbJ",
        "outputId": "dc9b0a33-90a1-4171-cdfd-bc9e15da3a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:430: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Last weekend, I went on a trip to the beach with my family, and it was really wonderful.</s>\n",
            "<s>  However, I was faced with many challenges when I started the new project, but I succeeded in solving them creatively.</s>\n",
            "<s> I've been reading books every night, and I'm learning new knowledge every day.</s>\n",
            "<s> 3 months ago, I started cooking as a new hobby, and I'm trying out various recipes to make it taste good. I'm enjoying it with my family.</s>\n",
            "<s> Summer vacation was spent traveling to several countries, and making new friends.</s>\n",
            "<s> 3 months ago, I started to exercise regularly, and my body and mood are both fine.</s>\n",
            "<s>  regards to the upcoming project, I'd like to share and work together with every members to achieve the best results.</s>\n",
            "<s> Music has always been my interest, so I started practicing the guitar by myself, and I also enjoy various kinds of music.</s>\n",
            "<s>  Fridays and Saturdays are for me to study new technologies and self-development.</s>\n",
            "<s>  Since I'm planning a ski trip with my family this winter, I'm looking forward to it.</s>\n",
            "0.42079962967233425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU2"
      ],
      "metadata": {
        "id": "TVxF35Wp98jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_my_translate = []\n",
        "for sentence in gpt_sentences :\n",
        "  print(gen(sentence))\n",
        "  gpt_my_translate.append(gen(sentence))\n",
        "\n",
        "gpt_my_translate = [sentence.replace(\"<s>\", '').replace(\"</s>\", '').strip() for sentence in gpt_my_translate]\n",
        "\n",
        "bleu_list = []\n",
        "for i in range(10) :\n",
        "  c = gpt_google_translate[i].split()\n",
        "  t = gpt_my_translate[i].split()\n",
        "\n",
        "  bleu_list.append(sentence_bleu([c], t, weights = (1, 0, 0, 0)))\n",
        "\n",
        "bleu_score = np.mean(bleu_list)\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "id": "Sq2ZeWTS97xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ë°©ë²•3\n",
        "í›ˆë ¨ ë°ì´í„° ì¤‘ ì„ì˜ë¡œ 20ë¬¸ì¥ ì„ íƒ"
      ],
      "metadata": {
        "id": "ba7XZsVA-Dyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/conversational-prompt/conversational.json\", 'r', encoding = \"utf-8\") as f:\n",
        "  json_data = json.load(f)"
      ],
      "metadata": {
        "id": "cv_wFPbzTSo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random.seed(1234)\n",
        "sample_num = 20\n",
        "indices = np.arange(200)\n",
        "np.random.shuffle(indices)\n",
        "indices = indices[0:sample_num]\n",
        "\n",
        "korean_sentences = []\n",
        "english_sentences = []\n",
        "\n",
        "for i in indices :\n",
        "  text = json_data[i][\"text\"]\n",
        "  korean_start = text.find(\"Korean : \") + len(\"Korean : \")\n",
        "  english_start = text.find(\"English : \") + len(\"English : \")\n",
        "\n",
        "  korean_end = text.find(\" ### English : \")\n",
        "  english_end = len(text)\n",
        "\n",
        "  korean_sentence = text[korean_start : korean_end] # ì…ë ¥ë¬¸ì¥\n",
        "  english_sentence = text[english_start : english_end] # ì •ë‹µ\n",
        "\n",
        "  korean_sentences.append(korean_sentence)\n",
        "  english_sentences.append(english_sentence)\n",
        "\n",
        "print(korean_sentences)\n",
        "print(english_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfa9vMBNXKW1",
        "outputId": "bbde1064-75b5-440f-e9d5-90c84a878434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ì§€ë‚œì£¼ì— ìš”ì²­í•œ ì§ì›ìš© ì„¤ë¬¸ì§€ëŠ” ë‹¤ ì œì¶œí–ˆë‚˜ìš”?', 'ë‹¹ì¥ì€ ë§¤ì¶œì´ ëŠ˜ê² ì§€ë§Œ, ì¥ê¸°ì ìœ¼ë¡œëŠ” ë¶€ì •ì ì´ë¼ê³  ìƒê°í•´ìš”.', 'ì €ëŠ” ë°˜ëŒ€í•©ë‹ˆë‹¤. ê°‘ìê¸° ì•¼ê·¼ì´ ëŠ˜ë©´ ì§ì›ë“¤ì´ í˜ë“¤ ê±°ì—ìš”.', 'ì™œ ì˜¤ëŠ˜ ì˜¤ì „ì— ì‹¤ì‹œëœ êµìœ¡ì— ì°¸ì—¬í•˜ì§€ ì•Šì•˜ë‚˜ìš”?', 'ì˜ëë„¤ìš”! ì§€ê¸ˆ ë°”ë¡œ 1í˜¸ì‹¤ì—ì„œ 5í˜¸ì‹¤ë¡œ ë³€ê²½í•´ë“œë¦´ê²Œìš”.', 'ì €ëŠ” ì „í™”ì™€ ì´ë©”ì¼ë§Œìœ¼ë¡œë„ ì¶©ë¶„í•˜ë‹¤ê³  ìƒê°í•´ìš”.', 'ë°˜ëŒ€ë¡œ ë§í•˜ìë©´ ì˜¤ë«ë™ì•ˆ ì§€ì í•˜ì§€ ì•Šì•˜ë˜ ê³ ê°ë“¤ë„ ìˆë‹¤ëŠ” ê±°ë„¤ìš”.', 'ì‚¬ë¬´ìš©í’ˆì´ë‚˜ ê°€êµ¬ ì¤‘ ê°€ì¥ ë¨¼ì € ë°”ê¿”ì•¼ í•  ê²ƒì€ ë­ë¼ê³  ìƒê°í•˜ë‚˜ìš”?', 'ë„¤, ë‘ ë‚ ì§œ ëª¨ë‘ ê´œì°®ìœ¼ë‹ˆ í¸í•˜ì‹¤ ë•Œë¡œ ì¡ìœ¼ì„¸ìš”.', 'ë„¤, ì—¬ëŸ¬ ëª…ì˜ ì•„ì´ë””ì–´ê°€ ìˆìœ¼ë©´ ê²°ê³¼ë„ ë” ì¢‹ê² ì£ .', 'ì£¼ë¬¸ì€ ê°€ëŠ¥í•˜ì§€ë§Œ, ìˆ˜ë ¹ì€ 2ë‹¬ ì •ë„ ê±¸ë¦´ ê²ƒ ê°™ë„¤ìš”.', 'ìš°ë¦¬ ì¡ì§€ì˜ êµ¬ë…ì„ ì·¨ì†Œí•˜ëŠ” ê°€ì¥ í° ì´ìœ ëŠ” ë­”ê°€ìš”?', 'ì‘ë…„ ëŒ€ë¹„ ì˜¬í•´ ë§¤ì¶œì´ 2ë°° ì˜¬ëë‹¤ê³  ë³´ê³ í•´ë„ ë ê¹Œìš”?', 'í•˜ì§€ë§Œ ê·¸ëƒ¥ ë‘ê¸°ì—ëŠ” ë„ˆë¬´ í° ê³µê°„ì¸ë°, ë¬´ìŠ¨ ì´ìœ ê°€ ìˆë‚˜ìš”?', 'í•˜ì§€ë§Œ ìµœê·¼ ë§ì€ ê³ ê°ì´ ëŠ˜ ê°™ì€ ë””ìì¸ì„ ì§€ì í•˜ê³  ìˆì–´ìš”.', 'ì €ëŠ” ì¢€ ë°˜ëŒ€ì˜ê²¬ì¸ë°, íšŒì˜ê°€ ì—†ìœ¼ë©´ ì§„í–‰ì´ ë ê¹Œìš”?', 'ì´ë¯¸ ì—¬ëŸ¬ ë²ˆ ê²€í† ë¥¼ ë§ˆì³¤ì§€ë§Œ, ì›í•œë‹¤ë©´ ë‹¤ì‹œ í•˜ì…”ë„ ë¼ìš”.', 'ì•„ë§ˆ ë‹¤ìŒ ì£¼ í™”ìš”ì¼ ì¼ ê±°ì—ìš”. ë‹¤ì‹œ í™•ì¸í•´ë³¼ê²Œìš”.', 'ë„¤, ì§ì›ë“¤ê³¼ ìƒì˜í•´ì„œ í›„ë³´ë¥¼ ëª‡ ëª… ì •í•´ë³¼ê²Œìš”.', 'ìƒˆë¡œìš´ ë³µì‚¬ê¸°ê°€ ìƒê¸¸ ë•Œê¹Œì§€ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•  ì§€ ëª¨ë¥´ê² ë„¤ìš”.']\n",
            "['Did everyone submit the employee survey from last week?', 'It may boost sales temporarily, but it would have negative effects in the long term.', 'I disagree. The sudden increase in overtime work will give the employees a hard time.', \"Why didn't you join the education program this morning?\", \"Wonderful! I'll reserve room 5 instead of room 1 for you.\", 'I think the email and the phone are enough, already.', 'In other words, a lot of customers have not been complaining about the design.', 'Which do you think we need to change first, the furniture or the stationery?', 'Sure, either one is fine for me. Just pick a date you like.', \"Yes, I think they'll come up with some great ideas.\", 'The order is acceptable, but it would take about 2 months until the delivery.', 'What is the biggest reason for canceling the subscription to our magazine?', 'Is it okay to report that the sales increased by 2 times compared to last year?', \"But it's a huge space being wasted. Is there a particular reason for it?\", 'But lately, a lot of customers started complaining about our consistent design.', \"I disagree. I don't think we can do without general meetings.\", 'I have checked them many times already, but you may do it again if you want to.', \"Probably next Tuesday. I'll double check.\", \"Yes, I'll talk to the employees and find several candidates.\", \"I don't know what to do until we get a new copy machine.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_sentences = []\n",
        "for i in range(sample_num):\n",
        "  print(\"ì…ë ¥ ë¬¸ì¥ : \", korean_sentences[i])\n",
        "  print(\"ë²ˆì—­ ë¬¸ì¥ : \", gen(korean_sentences[i]))\n",
        "  print(\"ì •ë‹µ ë¬¸ì¥ : \", english_sentences[i])\n",
        "  print(\"\\n---------------------------------------\\n\")\n",
        "  translate_sentences.append(gen(korean_sentences[i]))\n",
        "\n",
        "translate_sentences = [t.replace(\"<s>\", '').replace(\"</s>\", '').strip() for t in translate_sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb2tCIuuYceX",
        "outputId": "060d95b6-0ff6-4c11-e3ce-69c0978cf545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ ë¬¸ì¥ :  ì§€ë‚œì£¼ì— ìš”ì²­í•œ ì§ì›ìš© ì„¤ë¬¸ì§€ëŠ” ë‹¤ ì œì¶œí–ˆë‚˜ìš”?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:430: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë²ˆì—­ ë¬¸ì¥ :  <s> Did you send the employee survey from last week?</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  Did everyone submit the employee survey from last week?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ë‹¹ì¥ì€ ë§¤ì¶œì´ ëŠ˜ê² ì§€ë§Œ, ì¥ê¸°ì ìœ¼ë¡œëŠ” ë¶€ì •ì ì´ë¼ê³  ìƒê°í•´ìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  It would increase the sales in the short term, but it would be counterproductive in the long term.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  It may boost sales temporarily, but it would have negative effects in the long term.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì €ëŠ” ë°˜ëŒ€í•©ë‹ˆë‹¤. ê°‘ìê¸° ì•¼ê·¼ì´ ëŠ˜ë©´ ì§ì›ë“¤ì´ í˜ë“¤ ê±°ì—ìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  I disagree. It would be too difficult for the employees suddenly working overtime.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  I disagree. The sudden increase in overtime work will give the employees a hard time.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì™œ ì˜¤ëŠ˜ ì˜¤ì „ì— ì‹¤ì‹œëœ êµìœ¡ì— ì°¸ì—¬í•˜ì§€ ì•Šì•˜ë‚˜ìš”?\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  Why didn't I participate in the education this morning?</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  Why didn't you join the education program this morning?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì˜ëë„¤ìš”! ì§€ê¸ˆ ë°”ë¡œ 1í˜¸ì‹¤ì—ì„œ 5í˜¸ì‹¤ë¡œ ë³€ê²½í•´ë“œë¦´ê²Œìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s> Great! I'll change it to room 5 right now.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  Wonderful! I'll reserve room 5 instead of room 1 for you.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì €ëŠ” ì „í™”ì™€ ì´ë©”ì¼ë§Œìœ¼ë¡œë„ ì¶©ë¶„í•˜ë‹¤ê³  ìƒê°í•´ìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  I think it's fine with just phone and email.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  I think the email and the phone are enough, already.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ë°˜ëŒ€ë¡œ ë§í•˜ìë©´ ì˜¤ë«ë™ì•ˆ ì§€ì í•˜ì§€ ì•Šì•˜ë˜ ê³ ê°ë“¤ë„ ìˆë‹¤ëŠ” ê±°ë„¤ìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  However, there are some customers we didn't know about for a long time.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  In other words, a lot of customers have not been complaining about the design.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì‚¬ë¬´ìš©í’ˆì´ë‚˜ ê°€êµ¬ ì¤‘ ê°€ì¥ ë¨¼ì € ë°”ê¿”ì•¼ í•  ê²ƒì€ ë­ë¼ê³  ìƒê°í•˜ë‚˜ìš”?\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  What do you think should be changed first among the furniture and the stationery?</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  Which do you think we need to change first, the furniture or the stationery?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ë„¤, ë‘ ë‚ ì§œ ëª¨ë‘ ê´œì°®ìœ¼ë‹ˆ í¸í•˜ì‹¤ ë•Œë¡œ ì¡ìœ¼ì„¸ìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  Sure, either one is fine for me. Just pick a date you want.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  Sure, either one is fine for me. Just pick a date you like.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ë„¤, ì—¬ëŸ¬ ëª…ì˜ ì•„ì´ë””ì–´ê°€ ìˆìœ¼ë©´ ê²°ê³¼ë„ ë” ì¢‹ê² ì£ .\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s> Yes, I think the result would be better if several ideas are combined.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  Yes, I think they'll come up with some great ideas.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì£¼ë¬¸ì€ ê°€ëŠ¥í•˜ì§€ë§Œ, ìˆ˜ë ¹ì€ 2ë‹¬ ì •ë„ ê±¸ë¦´ ê²ƒ ê°™ë„¤ìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  It's possible, but it'll take about 2 months to deliver.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  The order is acceptable, but it would take about 2 months until the delivery.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ìš°ë¦¬ ì¡ì§€ì˜ êµ¬ë…ì„ ì·¨ì†Œí•˜ëŠ” ê°€ì¥ í° ì´ìœ ëŠ” ë­”ê°€ìš”?\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  What is the biggest reason why you want to cancel the subscription to our magazine?</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  What is the biggest reason for canceling the subscription to our magazine?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì‘ë…„ ëŒ€ë¹„ ì˜¬í•´ ë§¤ì¶œì´ 2ë°° ì˜¬ëë‹¤ê³  ë³´ê³ í•´ë„ ë ê¹Œìš”?\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  Would it be okay if I report that the sales increased by twice compared to last year?</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  Is it okay to report that the sales increased by 2 times compared to last year?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  í•˜ì§€ë§Œ ê·¸ëƒ¥ ë‘ê¸°ì—ëŠ” ë„ˆë¬´ í° ê³µê°„ì¸ë°, ë¬´ìŠ¨ ì´ìœ ê°€ ìˆë‚˜ìš”?\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  However, it's too big of an area, why?</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  But it's a huge space being wasted. Is there a particular reason for it?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  í•˜ì§€ë§Œ ìµœê·¼ ë§ì€ ê³ ê°ì´ ëŠ˜ ê°™ì€ ë””ìì¸ì„ ì§€ì í•˜ê³  ìˆì–´ìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  However, many customers have been complaining about the constant design.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  But lately, a lot of customers started complaining about our consistent design.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì €ëŠ” ì¢€ ë°˜ëŒ€ì˜ê²¬ì¸ë°, íšŒì˜ê°€ ì—†ìœ¼ë©´ ì§„í–‰ì´ ë ê¹Œìš”?\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  I'd say the opposite, wouldn't it be impossible to finish the work without the meetings?</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  I disagree. I don't think we can do without general meetings.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì´ë¯¸ ì—¬ëŸ¬ ë²ˆ ê²€í† ë¥¼ ë§ˆì³¤ì§€ë§Œ, ì›í•œë‹¤ë©´ ë‹¤ì‹œ í•˜ì…”ë„ ë¼ìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  It has been reviewed several times, but if you want, do it again.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  I have checked them many times already, but you may do it again if you want to.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ì•„ë§ˆ ë‹¤ìŒ ì£¼ í™”ìš”ì¼ ì¼ ê±°ì—ìš”. ë‹¤ì‹œ í™•ì¸í•´ë³¼ê²Œìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  Probably next Tuesday. I'll double check it.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  Probably next Tuesday. I'll double check.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ë„¤, ì§ì›ë“¤ê³¼ ìƒì˜í•´ì„œ í›„ë³´ë¥¼ ëª‡ ëª… ì •í•´ë³¼ê²Œìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  Sure, I'll talk to the employees and find a few candidates.</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  Yes, I'll talk to the employees and find several candidates.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "ì…ë ¥ ë¬¸ì¥ :  ìƒˆë¡œìš´ ë³µì‚¬ê¸°ê°€ ìƒê¸¸ ë•Œê¹Œì§€ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•  ì§€ ëª¨ë¥´ê² ë„¤ìš”.\n",
            "ë²ˆì—­ ë¬¸ì¥ :  <s>  How should we do until the new copier is installed?</s>\n",
            "ì •ë‹µ ë¬¸ì¥ :  I don't know what to do until we get a new copy machine.\n",
            "\n",
            "---------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_sentences)\n",
        "print(translate_sentences)\n",
        "print(korean_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzqi9A2QekeY",
        "outputId": "29669a44-1b96-477e-993f-1e2bb1306a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Did everyone submit the employee survey from last week?', 'It may boost sales temporarily, but it would have negative effects in the long term.', 'I disagree. The sudden increase in overtime work will give the employees a hard time.', \"Why didn't you join the education program this morning?\", \"Wonderful! I'll reserve room 5 instead of room 1 for you.\", 'I think the email and the phone are enough, already.', 'In other words, a lot of customers have not been complaining about the design.', 'Which do you think we need to change first, the furniture or the stationery?', 'Sure, either one is fine for me. Just pick a date you like.', \"Yes, I think they'll come up with some great ideas.\", 'The order is acceptable, but it would take about 2 months until the delivery.', 'What is the biggest reason for canceling the subscription to our magazine?', 'Is it okay to report that the sales increased by 2 times compared to last year?', \"But it's a huge space being wasted. Is there a particular reason for it?\", 'But lately, a lot of customers started complaining about our consistent design.', \"I disagree. I don't think we can do without general meetings.\", 'I have checked them many times already, but you may do it again if you want to.', \"Probably next Tuesday. I'll double check.\", \"Yes, I'll talk to the employees and find several candidates.\", \"I don't know what to do until we get a new copy machine.\"]\n",
            "['Did you send the employee survey from last week?', 'It would increase the sales in the short term, but it would be counterproductive in the long term.', 'I disagree. It would be too difficult for the employees suddenly working overtime.', \"Why didn't I participate in the education this morning?\", \"Great! I'll change it to room 5 right now.\", \"I think it's fine with just phone and email.\", \"However, there are some customers we didn't know about for a long time.\", 'What do you think should be changed first among the furniture and the stationery?', 'Sure, either one is fine for me. Just pick a date you want.', 'Yes, I think the result would be better if several ideas are combined.', \"It's possible, but it'll take about 2 months to deliver.\", 'What is the biggest reason why you want to cancel the subscription to our magazine?', 'Would it be okay if I report that the sales increased by twice compared to last year?', \"However, it's too big of an area, why?\", 'However, many customers have been complaining about the constant design.', \"I'd say the opposite, wouldn't it be impossible to finish the work without the meetings?\", 'It has been reviewed several times, but if you want, do it again.', \"Probably next Tuesday. I'll double check it.\", \"Sure, I'll talk to the employees and find a few candidates.\", 'How should we do until the new copier is installed?']\n",
            "['ì§€ë‚œì£¼ì— ìš”ì²­í•œ ì§ì›ìš© ì„¤ë¬¸ì§€ëŠ” ë‹¤ ì œì¶œí–ˆë‚˜ìš”?', 'ë‹¹ì¥ì€ ë§¤ì¶œì´ ëŠ˜ê² ì§€ë§Œ, ì¥ê¸°ì ìœ¼ë¡œëŠ” ë¶€ì •ì ì´ë¼ê³  ìƒê°í•´ìš”.', 'ì €ëŠ” ë°˜ëŒ€í•©ë‹ˆë‹¤. ê°‘ìê¸° ì•¼ê·¼ì´ ëŠ˜ë©´ ì§ì›ë“¤ì´ í˜ë“¤ ê±°ì—ìš”.', 'ì™œ ì˜¤ëŠ˜ ì˜¤ì „ì— ì‹¤ì‹œëœ êµìœ¡ì— ì°¸ì—¬í•˜ì§€ ì•Šì•˜ë‚˜ìš”?', 'ì˜ëë„¤ìš”! ì§€ê¸ˆ ë°”ë¡œ 1í˜¸ì‹¤ì—ì„œ 5í˜¸ì‹¤ë¡œ ë³€ê²½í•´ë“œë¦´ê²Œìš”.', 'ì €ëŠ” ì „í™”ì™€ ì´ë©”ì¼ë§Œìœ¼ë¡œë„ ì¶©ë¶„í•˜ë‹¤ê³  ìƒê°í•´ìš”.', 'ë°˜ëŒ€ë¡œ ë§í•˜ìë©´ ì˜¤ë«ë™ì•ˆ ì§€ì í•˜ì§€ ì•Šì•˜ë˜ ê³ ê°ë“¤ë„ ìˆë‹¤ëŠ” ê±°ë„¤ìš”.', 'ì‚¬ë¬´ìš©í’ˆì´ë‚˜ ê°€êµ¬ ì¤‘ ê°€ì¥ ë¨¼ì € ë°”ê¿”ì•¼ í•  ê²ƒì€ ë­ë¼ê³  ìƒê°í•˜ë‚˜ìš”?', 'ë„¤, ë‘ ë‚ ì§œ ëª¨ë‘ ê´œì°®ìœ¼ë‹ˆ í¸í•˜ì‹¤ ë•Œë¡œ ì¡ìœ¼ì„¸ìš”.', 'ë„¤, ì—¬ëŸ¬ ëª…ì˜ ì•„ì´ë””ì–´ê°€ ìˆìœ¼ë©´ ê²°ê³¼ë„ ë” ì¢‹ê² ì£ .', 'ì£¼ë¬¸ì€ ê°€ëŠ¥í•˜ì§€ë§Œ, ìˆ˜ë ¹ì€ 2ë‹¬ ì •ë„ ê±¸ë¦´ ê²ƒ ê°™ë„¤ìš”.', 'ìš°ë¦¬ ì¡ì§€ì˜ êµ¬ë…ì„ ì·¨ì†Œí•˜ëŠ” ê°€ì¥ í° ì´ìœ ëŠ” ë­”ê°€ìš”?', 'ì‘ë…„ ëŒ€ë¹„ ì˜¬í•´ ë§¤ì¶œì´ 2ë°° ì˜¬ëë‹¤ê³  ë³´ê³ í•´ë„ ë ê¹Œìš”?', 'í•˜ì§€ë§Œ ê·¸ëƒ¥ ë‘ê¸°ì—ëŠ” ë„ˆë¬´ í° ê³µê°„ì¸ë°, ë¬´ìŠ¨ ì´ìœ ê°€ ìˆë‚˜ìš”?', 'í•˜ì§€ë§Œ ìµœê·¼ ë§ì€ ê³ ê°ì´ ëŠ˜ ê°™ì€ ë””ìì¸ì„ ì§€ì í•˜ê³  ìˆì–´ìš”.', 'ì €ëŠ” ì¢€ ë°˜ëŒ€ì˜ê²¬ì¸ë°, íšŒì˜ê°€ ì—†ìœ¼ë©´ ì§„í–‰ì´ ë ê¹Œìš”?', 'ì´ë¯¸ ì—¬ëŸ¬ ë²ˆ ê²€í† ë¥¼ ë§ˆì³¤ì§€ë§Œ, ì›í•œë‹¤ë©´ ë‹¤ì‹œ í•˜ì…”ë„ ë¼ìš”.', 'ì•„ë§ˆ ë‹¤ìŒ ì£¼ í™”ìš”ì¼ ì¼ ê±°ì—ìš”. ë‹¤ì‹œ í™•ì¸í•´ë³¼ê²Œìš”.', 'ë„¤, ì§ì›ë“¤ê³¼ ìƒì˜í•´ì„œ í›„ë³´ë¥¼ ëª‡ ëª… ì •í•´ë³¼ê²Œìš”.', 'ìƒˆë¡œìš´ ë³µì‚¬ê¸°ê°€ ìƒê¸¸ ë•Œê¹Œì§€ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•  ì§€ ëª¨ë¥´ê² ë„¤ìš”.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU3\n",
        "Llama2 íŒŒì¸íŠœë‹ ëª¨ë¸ì˜ BLEU score ì¸¡ì •"
      ],
      "metadata": {
        "id": "0esq1iTV-L-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# english_sentences : ì •ë‹µë¬¸ì¥\n",
        "# translate_sentences : ë²ˆì—­ë¬¸ì¥\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "bleu_list = []\n",
        "for i in range(sample_num) :\n",
        "  correct = english_sentences[i].split()\n",
        "  translate = translate_sentences[i].split()\n",
        "\n",
        "  bleu_list.append(sentence_bleu([correct], translate, weights = (1, 0, 0, 0)))\n",
        "\n",
        "bleu_score = np.mean(bleu_list)\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyGGtFwJWj_i",
        "outputId": "460e022c-78af-458e-d791-b537371c0eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4460977814030677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "í›ˆë ¨ë°ì´í„°ì—ì„œ ì„ íƒí•œ í•œêµ­ì–´ ë¬¸ì¥ì„ êµ¬ê¸€ ë²ˆì—­ê¸°ë¡œ ë²ˆì—­í•œ ë’¤, BLEU scoreë¥¼ ì¸¡ì •"
      ],
      "metadata": {
        "id": "YPJbDbqk-T82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_translate = [\"Have you submitted all of the employee questionnaires requested last week?\",\n",
        "                    \"Sales may increase for now, but I think it will be negative in the long run.\",\n",
        "                    \"I am against it. If overtime suddenly increases, it will be difficult for employees.\",\n",
        "                    \"Why didn't you participate in the training held this morning?\",\n",
        "                    \"Good for you! I will change your room from room 1 to room 5 right now.\",\n",
        "                    \"I think phone calls and emails are enough.\",\n",
        "                    \"On the other hand, there are customers who have not pointed it out for a long time.\",\n",
        "                    \"What do you think should be changed first, office supplies or furniture?\",\n",
        "                    \"Yes, both dates are fine, so pick one that is convenient for you.\",\n",
        "                    \"Yes, the results will be better if multiple people have ideas.\",\n",
        "                    \"You can order, but it will likely take about 2 months to receive it.\",\n",
        "                    \"What's the biggest reason people cancel their subscription to our magazine?\",\n",
        "                    \"Can I report that this year's sales have doubled compared to last year?\",\n",
        "                    \"But it's too big of a space to just leave alone. What's the reason?\",\n",
        "                    \"But recently, many customers are always pointing out the same design.\",\n",
        "                    \"I am of the opposite opinion, but will it proceed without a meeting?\",\n",
        "                    \"I've already reviewed it several times, but you can do it again if you want.\",\n",
        "                    \"Probably next Tuesday. I'll check again.\",\n",
        "                    \"Yes, I will discuss with the staff and select a few candidates.\",\n",
        "                    \"I don't know what to do until I get a new copier.\"]\n",
        "\n",
        "# english_sentences : ì •ë‹µë¬¸ì¥\n",
        "# google_translate : êµ¬ê¸€ ë²ˆì—­ê¸°ë¬¸ì¥\n",
        "\n",
        "bleu_list = []\n",
        "for i in range(sample_num) :\n",
        "  correct = english_sentences[i].split()\n",
        "  translate = google_translate[i].split()\n",
        "\n",
        "  bleu_list.append(sentence_bleu([correct], translate, weights = (1, 0, 0, 0)))\n",
        "\n",
        "bleu_score = np.mean(bleu_list)\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlpgkB5Ogj7W",
        "outputId": "8054e3c7-dad3-41a0-d6b9-b71aea7bc52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3947945414541477\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM4csIKY3y1ZbSlg0RDo9qf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7dd2132503a4fc9b0e4c3fa94fecd06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7db01cd277b94160aef754289494575a",
              "IPY_MODEL_91b60ca6055148e8b894fcb66c5b54c0",
              "IPY_MODEL_c8b08b4c049144e295c9af350186fb21"
            ],
            "layout": "IPY_MODEL_9eccac0dc9e6480cb301259b95369c68"
          }
        },
        "7db01cd277b94160aef754289494575a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f91eb2dade4c1d86273e1084e73c9e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f495532cbd084f5f9eb79437b96405a4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "91b60ca6055148e8b894fcb66c5b54c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c27000ea89f44ac28d7a142dc4a32dde",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2d2832c14b048be9e9da83efadd77f9",
            "value": 14
          }
        },
        "c8b08b4c049144e295c9af350186fb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f2257da065149dbab8ffc5304957a12",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7b71dded5960402f8f4fa343e1249d65",
            "value": " 14/14 [01:06&lt;00:00,  4.62s/it]"
          }
        },
        "9eccac0dc9e6480cb301259b95369c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f91eb2dade4c1d86273e1084e73c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f495532cbd084f5f9eb79437b96405a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c27000ea89f44ac28d7a142dc4a32dde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d2832c14b048be9e9da83efadd77f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f2257da065149dbab8ffc5304957a12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b71dded5960402f8f4fa343e1249d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}