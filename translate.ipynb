{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swyoo5/LlamaFineTuningProject/blob/main/translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNHlnnVlEDrg",
        "outputId": "e7d350ac-8075-4093-8da4-8d7e27dde3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU :  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU : \", len(tf.config.experimental.list_physical_devices(\"GPU\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MoclIjZK1Zm",
        "outputId": "6204e8af-d024-4718-a111-ff7aa7d61f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May  9 06:36:06 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8              11W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# 엔비디아 gpu 드라이버와 관련된 시스템 및 하드웨어 정보를 조회\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m6CkJRBK7XY",
        "outputId": "09cae48e-fb76-4539-b99b-233fd6529fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/213.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/213.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/174.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q autotrain-advanced==0.6.80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_Lqt8wJK-xr",
        "outputId": "fb07e31d-5c98-4618-e67f-2f4c91801562"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "> \u001b[1mINFO    Installing latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest PyTorch\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest PyTorch\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!autotrain setup --update-torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTONUBYGe7NW"
      },
      "source": [
        "# 파인튜닝하기\n",
        "llm --train : train을 수행하는 llm 지정\n",
        "\n",
        "project-name : 학습과정이 저장될 폴더명\n",
        "\n",
        "data-path : 파인튜닝에 사용될 데이터의 경로, 해당파일 안에 json파일이 있어야 함(conversational1.json).\n",
        "\n",
        "text-column : 학습데이터에서 입력으로 사용할 열 지정\n",
        "\n",
        "peft : peft(parameter efficient fine tuning) 활성화\n",
        "\n",
        "quantization : 양자화 방법을 지정(int4 : 4비트 정수 양자화)\n",
        "\n",
        "lr : 학습률 지정\n",
        "\n",
        "batch-size : 배치 크기 설정. 한번에 처리할 샘플의 수\n",
        "\n",
        "epochs : 에폭수\n",
        "\n",
        "trainer : 학습에 사용할 트레이너(sft : supervised fine tuning)\n",
        "\n",
        "model_max_length : 모델이 처리할 수 있는 최대 입력 길이 지정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2z966NSLCVh",
        "outputId": "ac5d8ffd-0e07-4d49-c672-aeb4c1662c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "> \u001b[1mINFO    Running LLM\u001b[0m\n",
            "> \u001b[1mINFO    Params: Namespace(version=False, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model_ref=None, warmup_ratio=0.1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.0, max_grad_norm=1.0, add_eos_token=False, block_size=-1, peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, mixed_precision=None, quantization='int4', model_max_length=80, trainer='sft', target_modules=None, merge_adapter=False, use_flash_attention_2=False, dpo_beta=0.1, apply_chat_template=False, padding=None, train=True, deploy=False, inference=False, username=None, backend='local-cli', token=None, repo_id=None, push_to_hub=False, model='TinyPixel/Llama-2-7B-bf16-sharded', project_name='conversational-finetuning', seed=42, epochs=10, gradient_accumulation=1, disable_gradient_checkpointing=False, lr=0.0003, log='none', data_path='conversational-prompt', train_split='train', valid_split=None, batch_size=8, func=<function run_llm_command_factory at 0x7d73d2df6170>)\u001b[0m\n",
            "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
            "> \u001b[1mINFO    {\"model\":\"TinyPixel/Llama-2-7B-bf16-sharded\",\"project_name\":\"conversational-finetuning\",\"data_path\":\"conversational-prompt\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":-1,\"model_max_length\":80,\"padding\":null,\"trainer\":\"sft\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":false,\"mixed_precision\":null,\"lr\":0.0003,\"epochs\":10,\"batch_size\":8,\"warmup_ratio\":0.1,\"gradient_accumulation\":1,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.0,\"max_grad_norm\":1.0,\"seed\":42,\"apply_chat_template\":false,\"quantization\":\"int4\",\"target_modules\":null,\"merge_adapter\":false,\"peft\":true,\"lora_r\":16,\"lora_alpha\":32,\"lora_dropout\":0.05,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"prompt\",\"text_column\":\"text\",\"rejected_text_column\":\"rejected\",\"push_to_hub\":false,\"repo_id\":null,\"username\":null,\"token\":null}\u001b[0m\n",
            "> \u001b[1mINFO    ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'conversational-finetuning/training_params.json']\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-05-09 07:18:34\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mTrain data: Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 200\n",
            "})\u001b[0m\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-05-09 07:18:34\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
            "Loading checkpoint shards: 100% 14/14 [01:07<00:00,  4.83s/it]\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-05-09 07:19:43\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mUsing block size 80\u001b[0m\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-05-09 07:19:43\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m339\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "  0% 0/250 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (114 > 80). Running this sequence through the model will result in indexing errors\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 2.8454, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.2}\n",
            "{'loss': 2.8365, 'learning_rate': 0.00011999999999999999, 'epoch': 0.4}\n",
            "{'loss': 2.466, 'learning_rate': 0.00017999999999999998, 'epoch': 0.6}\n",
            "  6% 16/250 [02:12<32:14,  8.27s/it]Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "Process ForkProcess-2:\n",
            "Process ForkProcess-1:\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/autotrain\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/cli/autotrain.py\", line 48, in main\n",
            "    command.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/cli/run_llm.py\", line 383, in run\n",
            "    _ = project.create()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/project.py\", line 71, in create\n",
            "    return self.create_spaces()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/project.py\", line 64, in create_spaces\n",
            "    space_id = sr.prepare()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/backend.py\", line 103, in prepare\n",
            "    space_id = self._create_space()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/backend.py\", line 243, in _create_space\n",
            "    pid = local_runner.create()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/backend.py\", line 287, in create\n",
            "    training_pid = run_training(params, task_id, local=True, wait=self.wait)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/app_utils.py\", line 138, in run_training\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1222, in wait\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1017, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 634, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/__main__.py\", line 528, in <module>\n",
            "    train(_config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/common.py\", line 87, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/__main__.py\", line 475, in train\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 280, in train\n",
            "    output = super().train(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1854, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2737, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1905, in backward\n",
            "    loss.backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 522, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 266, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "  6% 16/250 [02:20<34:12,  8.77s/it]\n"
          ]
        }
      ],
      "source": [
        "!autotrain llm --train \\\n",
        "  --project-name \"conversational-finetuning\" \\\n",
        "  --model \"TinyPixel/Llama-2-7B-bf16-sharded\" \\\n",
        "  --data-path \"conversational-prompt\" \\\n",
        "  --text-column \"text\" \\\n",
        "  --peft \\\n",
        "  --quantization \"int4\" \\\n",
        "  --lr 3e-4 \\\n",
        "  --batch-size 8 \\\n",
        "  --epochs 10 \\\n",
        "  --trainer sft \\\n",
        "  --model_max_length 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eGmPk9zgs0A"
      },
      "source": [
        "# 체크포인트 파일 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvksWm9UV1X6"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "folder_name = \"conversational-finetuning\"\n",
        "zip_file_name = \"conversational-finetuning.zip\"\n",
        "shutil.make_archive(zip_file_name[:-4], \"zip\", folder_name)\n",
        "files.download(zip_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je2WsyGGrGy1"
      },
      "source": [
        "# Llama 번역작업"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMHe3-P4rJQV"
      },
      "outputs": [],
      "source": [
        "extract_folder_name = '.' # 현재 디렉토리\n",
        "\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref :\n",
        "  zip_ref.extractall(extract_folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d7dd2132503a4fc9b0e4c3fa94fecd06",
            "7db01cd277b94160aef754289494575a",
            "91b60ca6055148e8b894fcb66c5b54c0",
            "c8b08b4c049144e295c9af350186fb21",
            "9eccac0dc9e6480cb301259b95369c68",
            "10f91eb2dade4c1d86273e1084e73c9e",
            "f495532cbd084f5f9eb79437b96405a4",
            "c27000ea89f44ac28d7a142dc4a32dde",
            "c2d2832c14b048be9e9da83efadd77f9",
            "2f2257da065149dbab8ffc5304957a12",
            "7b71dded5960402f8f4fa343e1249d65"
          ]
        },
        "id": "BfvoDQPytIJs",
        "outputId": "b99ce0c3-e7e4-430c-a58d-2785095e0718"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7dd2132503a4fc9b0e4c3fa94fecd06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "model_id = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
        "peft_model_id = \"/content/checkpoint-250\" # peft를 적용할 체크포인트\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id) # peft를 위한 설정을 미리 학습된 체크포인트에서 로드한다.\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit = False, # 8비트 정수형으로 로드할지 여부\n",
        "    load_in_4bit = True, # 4비트 정수형으로 로드할지 여부\n",
        "    llm_int8_threshold = 6.0, # llm의 8비트 정수화를 위한 임계값\n",
        "    llm_int8_enable_fp32_cpu_offload = False, # llm 8비트 정수화를 위해 cpu offload 활성화 여부\n",
        "    llm_int8_has_fp16_weight = False, # 8비트 정수화에 fp16 가중치가 있는지 여부 설정\n",
        "    bnb_4bit_quant_type = \"nf4\", # 4비트 양자화 유형\n",
        "    bnb_4bit_use_double_quant = False, # 이중 양자화를 사용할지 여부\n",
        "    bnb_4bit_compute_dtype = \"float16\" # 4비트 연산에 사용할 데이터 유형\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config = bnb_config, device_map = {\"\":0}) # 사전학습된 모델을 로드, bits and bytes 알고리즘의 설정을 적용\n",
        "model = PeftModel.from_pretrained(model, peft_model_id) # 이전에 로드한 llm모델에 peft 적용\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id) # 토크나이저 로드, 사전학습 모델이 사용한 토크나이저\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mh_SOSyuV49"
      },
      "outputs": [],
      "source": [
        "prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\\n",
        "### Instruction : Korean and English sentences are given in order below. Translate Korean sentences into English sentences.\\\n",
        "### Korean : %s ### English : \"\n",
        "\n",
        "def gen(x) :\n",
        "  q = prompt % (x,)\n",
        "  gened = model.generate(\n",
        "      **tokenizer(\n",
        "          q,\n",
        "          return_tensors = \"pt\",\n",
        "          return_token_type_ids = False\n",
        "      ).to(\"cuda\"),\n",
        "      max_new_tokens = 128,\n",
        "      early_stopping = True,\n",
        "      do_sample = False\n",
        "  )\n",
        "  return tokenizer.decode(gened[0]).replace(q, '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n46rFmyWhQqZ"
      },
      "source": [
        "# Translate\n",
        "3가지 방법으로 실험을 진행했다.\n",
        "\n",
        "1. 훈련데이터 중 임의로 선택해 augment된 데이터 + 훈련 데이터에 포함되지 않은 데이터 중 임의로 선택\n",
        "\n",
        "  => 출력 문장과 정답 문장 간의 bleu score 측정\n",
        "\n",
        "2. ChatGPT가 임의로 생성한 문장\n",
        "  \n",
        "  => 구글 번역기에서 나온 결과를 정답 문장으로 해서 bleu score 측정\n",
        "\n",
        "3. 훈련 데이터에서 임의로 문장을 선택\n",
        "  \n",
        "  => 구글 번역기의 score와 Llama의 score 비교"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##방법1"
      ],
      "metadata": {
        "id": "46iju_CSNELL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터에서 임의로 선택 후 augment한 문장"
      ],
      "metadata": {
        "id": "3skfDHSs9pwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translate = [] # 모델 번역문장\n",
        "correct = [] # 정답 문장"
      ],
      "metadata": {
        "id": "zZkHnvBdjZaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e490FjEhSix",
        "outputId": "16bbc7ed-6309-4e7e-f820-f3bf99a53124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:430: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> 2 years ago, we started to work the same way as we are working now.</s>\n"
          ]
        }
      ],
      "source": [
        "# 원래문장 : 비용이 많이 들어서 지금과 같은 방법을 택했습니다.\n",
        "# 정답 : We chose the current way due to the cost.\n",
        "print(gen(\"비용 문제로 인해 지금처럼 접근했습니다.\"))\n",
        "translate.append(gen(\"비용 문제로 인해 지금처럼 접근했습니다.\"))\n",
        "correct.append(\"We chose the current way due to the cost.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2j2iXdnhy4y",
        "outputId": "fe6fd041-6896-4d48-f79b-6a2e2428bba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Below the email is raised with more specific details in smaller letters.</s>\n"
          ]
        }
      ],
      "source": [
        "# 원래문장 : 교육 안내 메일 하단에 작은 글씨로 적혀있어요.\n",
        "# 정답 : It was written in very small letters at the bottom of the notification email.\n",
        "print(gen(\"교육 안내 이메일의 아래에는 작은 글자로 더 많은 세부 사항이 기재되어 있습니다.\"))\n",
        "translate.append(gen(\"교육 안내 이메일의 아래에는 작은 글자로 더 많은 세부 사항이 기재되어 있습니다.\"))\n",
        "correct.append(\"It was written in very small letters at the bottom of the notification email.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJXOnuuGillG",
        "outputId": "b4f2656e-8cd1-4a0f-bc81-6161600e91cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  I'll ask them to leave that place, so that we can store the new products there.</s>\n"
          ]
        }
      ],
      "source": [
        "# 원래문장 : 네, 그쪽에 연락해서 신제품 보관을 위해 비워두도록 할게요.\n",
        "# 정답 : Sure, I'll contact them and clear the warehouse for it.\n",
        "print(gen(\"그곳에 연락해서 신제품 보관을 위해 그 곳을 비워두도록 조치하겠습니다.\"))\n",
        "translate.append(gen(\"그곳에 연락해서 신제품 보관을 위해 그 곳을 비워두도록 조치하겠습니다.\"))\n",
        "correct.append(\"Sure, I'll contact them and clear the warehouse for it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQXen-_2jb_j",
        "outputId": "6e411876-d9cd-4029-ef6e-07e0a128a23c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Where is the biggest warehouse of our company located?</s>\n"
          ]
        }
      ],
      "source": [
        "# 원래문장 : 회사 창고 중에 가장 큰 창고는 어느 지역의 창고인가요?\n",
        "# 정답 : Where is the biggest warehouse in our company located?\n",
        "print(gen(\"회사의 가장 큰 창고는 어느 도시에 위치하고 있나요?\"))\n",
        "translate.append(gen(\"회사의 가장 큰 창고는 어느 도시에 위치하고 있나요?\"))\n",
        "correct.append(\"Where is the biggest warehouse in our company located?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9-sKYbsjtz9",
        "outputId": "e777971e-57fb-47af-a8ba-5f1433225f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Since the number of people increased, we need a bigger room.</s>\n"
          ]
        }
      ],
      "source": [
        "# 원래문장 : 네, 참석예상인원이 많이 늘어서 넓은 공간이 필요해요.\n",
        "# 정답 : Yes, there will be more people than we expected, so we need a bigger room.\n",
        "print(gen(\"참석 예정 인원이 늘어나서 더 넓은 공간을 필요로 합니다.\"))\n",
        "translate.append(gen(\"참석 예정 인원이 늘어나서 더 넓은 공간을 필요로 합니다.\"))\n",
        "correct.append(\"Yes, there will be more people than we expected, so we need a bigger room.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJmkzYWskKIR",
        "outputId": "0f6c367c-b59b-45bb-afff-2321e523a487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Would it be possible to leave today or tomorrow?</s>\n"
          ]
        }
      ],
      "source": [
        "# 원래문장 : 혹시 오늘이나 내일 바로 출장을 떠날 수 있나요?\n",
        "# 정답 : Can you go on a business trip today or tomorrow?\n",
        "print(gen(\"오늘이나 내일 중에 출장을 떠나는 것이 가능한가요?\"))\n",
        "translate.append(gen(\"오늘이나 내일 중에 출장을 떠나는 것이 가능한가요?\"))\n",
        "correct.append(\"Can you go on a business trip today or tomorrow?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8evebVTklsL",
        "outputId": "75c553b8-9e83-44ca-e6ee-3fb4f84e075d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> 2 times already, the photocopier broke down, so I handed it to the maintenance.</s>\n"
          ]
        }
      ],
      "source": [
        "# 원래문장 : 복사기가 또 고장 나서 시설 관리팀에 연락했어요.\n",
        "# 정답 : The copy machine broke down again, so I called the maintenance.\n",
        "print(gen(\"또 다시 복사기가 고장 났으니 시설 관리팀에 접수했습니다.\"))\n",
        "translate.append(gen(\"또 다시 복사기가 고장 났으니 시설 관리팀에 접수했습니다.\"))\n",
        "correct.append(\"The copy machine broke down again, so I called the maintenance.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skDI0lamk73Y",
        "outputId": "e55285f1-af51-4ab9-cd2c-03ac17df8fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Would there be any other solutions besides adjusting the price?</s>\n"
          ]
        }
      ],
      "source": [
        "# 원래문장 : 그럼 제품 가격을 낮추는 것 외에 다른 방법이 있나요?\n",
        "# 정답 : Then, is there any other way except lowering the price?\n",
        "print(gen(\"제품 가격을 조정하는 것 외에도 다른 해결책이 있을까요?\"))\n",
        "translate.append(gen(\"제품 가격을 조정하는 것 외에도 다른 해결책이 있을까요?\"))\n",
        "correct.append(\"Then, is there any other way except lowering the price?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtQfrZiFlTjQ",
        "outputId": "e7957a79-296e-433a-d0fe-af1796e67408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  sales are steadily increasing, so I'll consider taking the big order.</s>\n"
          ]
        }
      ],
      "source": [
        "# 원래문장 : 꾸준히 매출이 증가하고 있는데 한 번에 주문하는 게 낫죠.\n",
        "# 정답 : The sales are consistently increasing, so it's better to order them at once.\n",
        "print(gen(\"매출이 꾸준히 늘어나고 있으니 대규모 주문을 고려해 보는 것이 좋겠어요.\"))\n",
        "translate.append(gen(\"매출이 꾸준히 늘어나고 있으니 대규모 주문을 고려해 보는 것이 좋겠어요.\"))\n",
        "correct.append(\"The sales are consistently increasing, so it's better to order them at once.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train 데이터에 포함되지 않은 문장을 입력"
      ],
      "metadata": {
        "id": "rHeem61XNUOC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgHTqTvCXd1v",
        "outputId": "f3b97a31-7e69-45d1-da43-264828ad72bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Great, but the new employees may have some problems.</s>\n"
          ]
        }
      ],
      "source": [
        "# 정답 : Sounds good, the new employees seem to have a hard time getting used to it.\n",
        "print(gen(\"좋아요, 안 그래도 신입 직원들은 조금 어려워하더라고요.\"))\n",
        "translate.append(gen(\"좋아요, 안 그래도 신입 직원들은 조금 어려워하더라고요.\"))\n",
        "correct.append(\"Sounds good, the new employees seem to have a hard time getting used to it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyJnu6MDGmq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7555007-45a6-4ac2-9601-0d840ced4387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Last semester, I had to take an intern, but it all went wrong.</s>\n"
          ]
        }
      ],
      "source": [
        "# 정답 : I had to do an intern last semester, but I totally failed.\n",
        "print(gen(\"저번 학기에 인턴을 해야 했는데, 다 떨어졌어.\"))\n",
        "translate.append(gen(\"저번 학기에 인턴을 해야 했는데, 다 떨어졌어.\"))\n",
        "correct.append(\"I had to do an intern last semester, but I totally failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpSaCwfdG4gI",
        "outputId": "0b20c5fb-e5c3-4840-a238-d8dd2d7022f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  I have a feeling that I made the time table too rigid.</s>\n"
          ]
        }
      ],
      "source": [
        "# 정답 : I think you overworked your timetable.\n",
        "print(gen(\"네가 시간표를 너무 무리하게 짠 것 같은 생각이 든다.\"))\n",
        "translate.append(gen(\"네가 시간표를 너무 무리하게 짠 것 같은 생각이 든다.\"))\n",
        "correct.append(\"I think you overworked your timetable.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chqxT3I1HZJJ",
        "outputId": "366eb27f-9d55-4ecb-ecc3-c4d051045264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>  Why did you register Chinese if you can't speak Chinese?</s>\n"
          ]
        }
      ],
      "source": [
        "# 정답 : Why did you enroll in Chinese class when you can't speak Chinese?\n",
        "print(gen(\"너 중국어도 못하면서 왜 중국어 수업을 등록했어?\"))\n",
        "translate.append(gen(\"너 중국어도 못하면서 왜 중국어 수업을 등록했어?\"))\n",
        "correct.append(\"Why did you enroll in Chinese class when you can't speak Chinese?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU1"
      ],
      "metadata": {
        "id": "R1aUn0hCnJl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translate = [t.replace(\"<s>\", '').replace(\"</s>\", '').strip() for t in translate]"
      ],
      "metadata": {
        "id": "bNs7JzXdoCXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWX85VJupCWE",
        "outputId": "9c61353e-70b8-4f24-fd0b-2ef3514ff955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2 years ago, we started to work the same way as we are working now.',\n",
              " 'Below the email is raised with more specific details in smaller letters.',\n",
              " \"I'll ask them to leave that place, so that we can store the new products there.\",\n",
              " 'Where is the biggest warehouse of our company located?',\n",
              " 'Since the number of people increased, we need a bigger room.',\n",
              " 'Would it be possible to leave today or tomorrow?',\n",
              " '2 times already, the photocopier broke down, so I handed it to the maintenance.',\n",
              " 'Would there be any other solutions besides adjusting the price?',\n",
              " \"sales are steadily increasing, so I'll consider taking the big order.\",\n",
              " 'Great, but the new employees may have some problems.',\n",
              " 'Last semester, I had to take an intern, but it all went wrong.',\n",
              " 'I have a feeling that I made the time table too rigid.',\n",
              " \"Why did you register Chinese if you can't speak Chinese?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmdqGY2apDqa",
        "outputId": "9a24dbf1-40e2-41be-f6c9-576df598d543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We chose the current way due to the cost.',\n",
              " 'It was written in very small letters at the bottom of the notification email.',\n",
              " \"Sure, I'll contact them and clear the warehouse for it.\",\n",
              " 'Where is the biggest warehouse in our company located?',\n",
              " 'Yes, there will be more people than we expected, so we need a bigger room.',\n",
              " 'Can you go on a business trip today or tomorrow?',\n",
              " 'The copy machine broke down again, so I called the maintenance.',\n",
              " 'Then, is there any other way except lowering the price?',\n",
              " \"The sales are consistently increasing, so it's better to order them at once.\",\n",
              " 'Sounds good, the new employees seem to have a hard time getting used to it.',\n",
              " 'I had to do an intern last semester, but I totally failed.',\n",
              " 'I think you overworked your timetable.',\n",
              " \"Why did you enroll in Chinese class when you can't speak Chinese?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "bleu_list = []\n",
        "for i in range(13) :\n",
        "  c = correct[i].split()\n",
        "  t = translate[i].split()\n",
        "\n",
        "  bleu_list.append(sentence_bleu([c], t, weights = (1, 0, 0, 0)))\n",
        "\n",
        "bleu_score = np.mean(bleu_list)\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiLYcNm3nQhD",
        "outputId": "9461805b-7379-424f-d022-79b564f353e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3602527603189402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 방법2\n",
        "ChatGPT가 생성한 임의의 문장"
      ],
      "metadata": {
        "id": "gaRliqgvR3Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_sentences = [\n",
        "    \"지난 주말에는 가족과 함께 해변으로 여행을 다녀왔는데, 정말로 시원하고 즐거운 시간을 보냈습니다.\",\n",
        "    \"새로운 프로젝트를 시작하면서 많은 도전에 직면했지만, 창의적인 해결책을 찾아내어 성공적으로 완료했습니다.\",\n",
        "    \"요즘에는 책을 읽는 것에 빠져 있어서 매일 밤 책을 읽으며 새로운 지식을 습득하고 있습니다.\",\n",
        "    \"최근에 새로운 취미로 요리를 시작했는데, 다양한 레시피를 실험하면서 맛있는 음식을 만들고 가족들과 함께 즐기고 있습니다\",\n",
        "    \"여름 휴가 때는 여러 나라를 여행하면서 다양한 문화를 경험하고 새로운 친구들을 사귀었습니다.\",\n",
        "    \"최근에는 건강을 위해 운동을 꾸준히 하고 있어서 몸도 건강하고 기분도 좋습니다.\",\n",
        "    \"다가오는 프로젝트에 대해 팀원들과 아이디어를 공유하고 협력하여 최상의 결과를 이끌어 내고자 합니다.\",\n",
        "    \"어릴 때부터 음악에 관심이 많아서 혼자 기타를 연습하고 다양한 음악을 즐기는 취미를 가지고 있습니다.\",\n",
        "    \"주말에는 시간을 활용하여 새로운 기술을 학습하고 자기 계발에 힘쓰고 있습니다.\",\n",
        "    \"다가오는 겨울 시즌에는 가족과 함께 스키 여행을 계획 중이어서 기대가 많이 됩니다.\"\n",
        "]\n",
        "\n",
        "gpt_google_translate = [\n",
        "    \"Last weekend I went on a trip to the beach with my family and had a really cool and fun time.\",\n",
        "    \"We faced many challenges when starting a new project, but we found creative solutions and completed it successfully.\",\n",
        "    \"These days, I am obsessed with reading books, and I am acquiring new knowledge by reading books every night.\",\n",
        "    \"I recently started cooking as a new hobby, experimenting with various recipes to make delicious food and enjoy it with my family.\",\n",
        "    \"During my summer vacation, I traveled to different countries, experienced different cultures and made new friends.\",\n",
        "    \"Recently, I have been exercising regularly for my health, so my body is healthy and I feel good.\",\n",
        "    \"We want to share ideas and collaborate with our team members on upcoming projects to achieve the best results.\",\n",
        "    \"I have been interested in music since I was young, and have hobbies of practicing guitar on my own and enjoying a variety of music.\",\n",
        "    \"On the weekends, I use my time to learn new skills and develop myself.\",\n",
        "    \"I'm planning a ski trip with my family this coming winter season, so I'm really looking forward to it.\"\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5E-tpsLRZbJ",
        "outputId": "dc9b0a33-90a1-4171-cdfd-bc9e15da3a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:430: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Last weekend, I went on a trip to the beach with my family, and it was really wonderful.</s>\n",
            "<s>  However, I was faced with many challenges when I started the new project, but I succeeded in solving them creatively.</s>\n",
            "<s> I've been reading books every night, and I'm learning new knowledge every day.</s>\n",
            "<s> 3 months ago, I started cooking as a new hobby, and I'm trying out various recipes to make it taste good. I'm enjoying it with my family.</s>\n",
            "<s> Summer vacation was spent traveling to several countries, and making new friends.</s>\n",
            "<s> 3 months ago, I started to exercise regularly, and my body and mood are both fine.</s>\n",
            "<s>  regards to the upcoming project, I'd like to share and work together with every members to achieve the best results.</s>\n",
            "<s> Music has always been my interest, so I started practicing the guitar by myself, and I also enjoy various kinds of music.</s>\n",
            "<s>  Fridays and Saturdays are for me to study new technologies and self-development.</s>\n",
            "<s>  Since I'm planning a ski trip with my family this winter, I'm looking forward to it.</s>\n",
            "0.42079962967233425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU2"
      ],
      "metadata": {
        "id": "TVxF35Wp98jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_my_translate = []\n",
        "for sentence in gpt_sentences :\n",
        "  print(gen(sentence))\n",
        "  gpt_my_translate.append(gen(sentence))\n",
        "\n",
        "gpt_my_translate = [sentence.replace(\"<s>\", '').replace(\"</s>\", '').strip() for sentence in gpt_my_translate]\n",
        "\n",
        "bleu_list = []\n",
        "for i in range(10) :\n",
        "  c = gpt_google_translate[i].split()\n",
        "  t = gpt_my_translate[i].split()\n",
        "\n",
        "  bleu_list.append(sentence_bleu([c], t, weights = (1, 0, 0, 0)))\n",
        "\n",
        "bleu_score = np.mean(bleu_list)\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "id": "Sq2ZeWTS97xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##방법3\n",
        "훈련 데이터 중 임의로 20문장 선택"
      ],
      "metadata": {
        "id": "ba7XZsVA-Dyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/conversational-prompt/conversational.json\", 'r', encoding = \"utf-8\") as f:\n",
        "  json_data = json.load(f)"
      ],
      "metadata": {
        "id": "cv_wFPbzTSo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random.seed(1234)\n",
        "sample_num = 20\n",
        "indices = np.arange(200)\n",
        "np.random.shuffle(indices)\n",
        "indices = indices[0:sample_num]\n",
        "\n",
        "korean_sentences = []\n",
        "english_sentences = []\n",
        "\n",
        "for i in indices :\n",
        "  text = json_data[i][\"text\"]\n",
        "  korean_start = text.find(\"Korean : \") + len(\"Korean : \")\n",
        "  english_start = text.find(\"English : \") + len(\"English : \")\n",
        "\n",
        "  korean_end = text.find(\" ### English : \")\n",
        "  english_end = len(text)\n",
        "\n",
        "  korean_sentence = text[korean_start : korean_end] # 입력문장\n",
        "  english_sentence = text[english_start : english_end] # 정답\n",
        "\n",
        "  korean_sentences.append(korean_sentence)\n",
        "  english_sentences.append(english_sentence)\n",
        "\n",
        "print(korean_sentences)\n",
        "print(english_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfa9vMBNXKW1",
        "outputId": "bbde1064-75b5-440f-e9d5-90c84a878434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['지난주에 요청한 직원용 설문지는 다 제출했나요?', '당장은 매출이 늘겠지만, 장기적으로는 부정적이라고 생각해요.', '저는 반대합니다. 갑자기 야근이 늘면 직원들이 힘들 거에요.', '왜 오늘 오전에 실시된 교육에 참여하지 않았나요?', '잘됐네요! 지금 바로 1호실에서 5호실로 변경해드릴게요.', '저는 전화와 이메일만으로도 충분하다고 생각해요.', '반대로 말하자면 오랫동안 지적하지 않았던 고객들도 있다는 거네요.', '사무용품이나 가구 중 가장 먼저 바꿔야 할 것은 뭐라고 생각하나요?', '네, 두 날짜 모두 괜찮으니 편하실 때로 잡으세요.', '네, 여러 명의 아이디어가 있으면 결과도 더 좋겠죠.', '주문은 가능하지만, 수령은 2달 정도 걸릴 것 같네요.', '우리 잡지의 구독을 취소하는 가장 큰 이유는 뭔가요?', '작년 대비 올해 매출이 2배 올랐다고 보고해도 될까요?', '하지만 그냥 두기에는 너무 큰 공간인데, 무슨 이유가 있나요?', '하지만 최근 많은 고객이 늘 같은 디자인을 지적하고 있어요.', '저는 좀 반대의견인데, 회의가 없으면 진행이 될까요?', '이미 여러 번 검토를 마쳤지만, 원한다면 다시 하셔도 돼요.', '아마 다음 주 화요일 일 거에요. 다시 확인해볼게요.', '네, 직원들과 상의해서 후보를 몇 명 정해볼게요.', '새로운 복사기가 생길 때까지는 어떻게 해야 할 지 모르겠네요.']\n",
            "['Did everyone submit the employee survey from last week?', 'It may boost sales temporarily, but it would have negative effects in the long term.', 'I disagree. The sudden increase in overtime work will give the employees a hard time.', \"Why didn't you join the education program this morning?\", \"Wonderful! I'll reserve room 5 instead of room 1 for you.\", 'I think the email and the phone are enough, already.', 'In other words, a lot of customers have not been complaining about the design.', 'Which do you think we need to change first, the furniture or the stationery?', 'Sure, either one is fine for me. Just pick a date you like.', \"Yes, I think they'll come up with some great ideas.\", 'The order is acceptable, but it would take about 2 months until the delivery.', 'What is the biggest reason for canceling the subscription to our magazine?', 'Is it okay to report that the sales increased by 2 times compared to last year?', \"But it's a huge space being wasted. Is there a particular reason for it?\", 'But lately, a lot of customers started complaining about our consistent design.', \"I disagree. I don't think we can do without general meetings.\", 'I have checked them many times already, but you may do it again if you want to.', \"Probably next Tuesday. I'll double check.\", \"Yes, I'll talk to the employees and find several candidates.\", \"I don't know what to do until we get a new copy machine.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_sentences = []\n",
        "for i in range(sample_num):\n",
        "  print(\"입력 문장 : \", korean_sentences[i])\n",
        "  print(\"번역 문장 : \", gen(korean_sentences[i]))\n",
        "  print(\"정답 문장 : \", english_sentences[i])\n",
        "  print(\"\\n---------------------------------------\\n\")\n",
        "  translate_sentences.append(gen(korean_sentences[i]))\n",
        "\n",
        "translate_sentences = [t.replace(\"<s>\", '').replace(\"</s>\", '').strip() for t in translate_sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb2tCIuuYceX",
        "outputId": "060d95b6-0ff6-4c11-e3ce-69c0978cf545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장 :  지난주에 요청한 직원용 설문지는 다 제출했나요?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:430: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "번역 문장 :  <s> Did you send the employee survey from last week?</s>\n",
            "정답 문장 :  Did everyone submit the employee survey from last week?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  당장은 매출이 늘겠지만, 장기적으로는 부정적이라고 생각해요.\n",
            "번역 문장 :  <s>  It would increase the sales in the short term, but it would be counterproductive in the long term.</s>\n",
            "정답 문장 :  It may boost sales temporarily, but it would have negative effects in the long term.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  저는 반대합니다. 갑자기 야근이 늘면 직원들이 힘들 거에요.\n",
            "번역 문장 :  <s>  I disagree. It would be too difficult for the employees suddenly working overtime.</s>\n",
            "정답 문장 :  I disagree. The sudden increase in overtime work will give the employees a hard time.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  왜 오늘 오전에 실시된 교육에 참여하지 않았나요?\n",
            "번역 문장 :  <s>  Why didn't I participate in the education this morning?</s>\n",
            "정답 문장 :  Why didn't you join the education program this morning?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  잘됐네요! 지금 바로 1호실에서 5호실로 변경해드릴게요.\n",
            "번역 문장 :  <s> Great! I'll change it to room 5 right now.</s>\n",
            "정답 문장 :  Wonderful! I'll reserve room 5 instead of room 1 for you.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  저는 전화와 이메일만으로도 충분하다고 생각해요.\n",
            "번역 문장 :  <s>  I think it's fine with just phone and email.</s>\n",
            "정답 문장 :  I think the email and the phone are enough, already.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  반대로 말하자면 오랫동안 지적하지 않았던 고객들도 있다는 거네요.\n",
            "번역 문장 :  <s>  However, there are some customers we didn't know about for a long time.</s>\n",
            "정답 문장 :  In other words, a lot of customers have not been complaining about the design.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  사무용품이나 가구 중 가장 먼저 바꿔야 할 것은 뭐라고 생각하나요?\n",
            "번역 문장 :  <s>  What do you think should be changed first among the furniture and the stationery?</s>\n",
            "정답 문장 :  Which do you think we need to change first, the furniture or the stationery?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  네, 두 날짜 모두 괜찮으니 편하실 때로 잡으세요.\n",
            "번역 문장 :  <s>  Sure, either one is fine for me. Just pick a date you want.</s>\n",
            "정답 문장 :  Sure, either one is fine for me. Just pick a date you like.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  네, 여러 명의 아이디어가 있으면 결과도 더 좋겠죠.\n",
            "번역 문장 :  <s> Yes, I think the result would be better if several ideas are combined.</s>\n",
            "정답 문장 :  Yes, I think they'll come up with some great ideas.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  주문은 가능하지만, 수령은 2달 정도 걸릴 것 같네요.\n",
            "번역 문장 :  <s>  It's possible, but it'll take about 2 months to deliver.</s>\n",
            "정답 문장 :  The order is acceptable, but it would take about 2 months until the delivery.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  우리 잡지의 구독을 취소하는 가장 큰 이유는 뭔가요?\n",
            "번역 문장 :  <s>  What is the biggest reason why you want to cancel the subscription to our magazine?</s>\n",
            "정답 문장 :  What is the biggest reason for canceling the subscription to our magazine?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  작년 대비 올해 매출이 2배 올랐다고 보고해도 될까요?\n",
            "번역 문장 :  <s>  Would it be okay if I report that the sales increased by twice compared to last year?</s>\n",
            "정답 문장 :  Is it okay to report that the sales increased by 2 times compared to last year?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  하지만 그냥 두기에는 너무 큰 공간인데, 무슨 이유가 있나요?\n",
            "번역 문장 :  <s>  However, it's too big of an area, why?</s>\n",
            "정답 문장 :  But it's a huge space being wasted. Is there a particular reason for it?\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  하지만 최근 많은 고객이 늘 같은 디자인을 지적하고 있어요.\n",
            "번역 문장 :  <s>  However, many customers have been complaining about the constant design.</s>\n",
            "정답 문장 :  But lately, a lot of customers started complaining about our consistent design.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  저는 좀 반대의견인데, 회의가 없으면 진행이 될까요?\n",
            "번역 문장 :  <s>  I'd say the opposite, wouldn't it be impossible to finish the work without the meetings?</s>\n",
            "정답 문장 :  I disagree. I don't think we can do without general meetings.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  이미 여러 번 검토를 마쳤지만, 원한다면 다시 하셔도 돼요.\n",
            "번역 문장 :  <s>  It has been reviewed several times, but if you want, do it again.</s>\n",
            "정답 문장 :  I have checked them many times already, but you may do it again if you want to.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  아마 다음 주 화요일 일 거에요. 다시 확인해볼게요.\n",
            "번역 문장 :  <s>  Probably next Tuesday. I'll double check it.</s>\n",
            "정답 문장 :  Probably next Tuesday. I'll double check.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  네, 직원들과 상의해서 후보를 몇 명 정해볼게요.\n",
            "번역 문장 :  <s>  Sure, I'll talk to the employees and find a few candidates.</s>\n",
            "정답 문장 :  Yes, I'll talk to the employees and find several candidates.\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "입력 문장 :  새로운 복사기가 생길 때까지는 어떻게 해야 할 지 모르겠네요.\n",
            "번역 문장 :  <s>  How should we do until the new copier is installed?</s>\n",
            "정답 문장 :  I don't know what to do until we get a new copy machine.\n",
            "\n",
            "---------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_sentences)\n",
        "print(translate_sentences)\n",
        "print(korean_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzqi9A2QekeY",
        "outputId": "29669a44-1b96-477e-993f-1e2bb1306a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Did everyone submit the employee survey from last week?', 'It may boost sales temporarily, but it would have negative effects in the long term.', 'I disagree. The sudden increase in overtime work will give the employees a hard time.', \"Why didn't you join the education program this morning?\", \"Wonderful! I'll reserve room 5 instead of room 1 for you.\", 'I think the email and the phone are enough, already.', 'In other words, a lot of customers have not been complaining about the design.', 'Which do you think we need to change first, the furniture or the stationery?', 'Sure, either one is fine for me. Just pick a date you like.', \"Yes, I think they'll come up with some great ideas.\", 'The order is acceptable, but it would take about 2 months until the delivery.', 'What is the biggest reason for canceling the subscription to our magazine?', 'Is it okay to report that the sales increased by 2 times compared to last year?', \"But it's a huge space being wasted. Is there a particular reason for it?\", 'But lately, a lot of customers started complaining about our consistent design.', \"I disagree. I don't think we can do without general meetings.\", 'I have checked them many times already, but you may do it again if you want to.', \"Probably next Tuesday. I'll double check.\", \"Yes, I'll talk to the employees and find several candidates.\", \"I don't know what to do until we get a new copy machine.\"]\n",
            "['Did you send the employee survey from last week?', 'It would increase the sales in the short term, but it would be counterproductive in the long term.', 'I disagree. It would be too difficult for the employees suddenly working overtime.', \"Why didn't I participate in the education this morning?\", \"Great! I'll change it to room 5 right now.\", \"I think it's fine with just phone and email.\", \"However, there are some customers we didn't know about for a long time.\", 'What do you think should be changed first among the furniture and the stationery?', 'Sure, either one is fine for me. Just pick a date you want.', 'Yes, I think the result would be better if several ideas are combined.', \"It's possible, but it'll take about 2 months to deliver.\", 'What is the biggest reason why you want to cancel the subscription to our magazine?', 'Would it be okay if I report that the sales increased by twice compared to last year?', \"However, it's too big of an area, why?\", 'However, many customers have been complaining about the constant design.', \"I'd say the opposite, wouldn't it be impossible to finish the work without the meetings?\", 'It has been reviewed several times, but if you want, do it again.', \"Probably next Tuesday. I'll double check it.\", \"Sure, I'll talk to the employees and find a few candidates.\", 'How should we do until the new copier is installed?']\n",
            "['지난주에 요청한 직원용 설문지는 다 제출했나요?', '당장은 매출이 늘겠지만, 장기적으로는 부정적이라고 생각해요.', '저는 반대합니다. 갑자기 야근이 늘면 직원들이 힘들 거에요.', '왜 오늘 오전에 실시된 교육에 참여하지 않았나요?', '잘됐네요! 지금 바로 1호실에서 5호실로 변경해드릴게요.', '저는 전화와 이메일만으로도 충분하다고 생각해요.', '반대로 말하자면 오랫동안 지적하지 않았던 고객들도 있다는 거네요.', '사무용품이나 가구 중 가장 먼저 바꿔야 할 것은 뭐라고 생각하나요?', '네, 두 날짜 모두 괜찮으니 편하실 때로 잡으세요.', '네, 여러 명의 아이디어가 있으면 결과도 더 좋겠죠.', '주문은 가능하지만, 수령은 2달 정도 걸릴 것 같네요.', '우리 잡지의 구독을 취소하는 가장 큰 이유는 뭔가요?', '작년 대비 올해 매출이 2배 올랐다고 보고해도 될까요?', '하지만 그냥 두기에는 너무 큰 공간인데, 무슨 이유가 있나요?', '하지만 최근 많은 고객이 늘 같은 디자인을 지적하고 있어요.', '저는 좀 반대의견인데, 회의가 없으면 진행이 될까요?', '이미 여러 번 검토를 마쳤지만, 원한다면 다시 하셔도 돼요.', '아마 다음 주 화요일 일 거에요. 다시 확인해볼게요.', '네, 직원들과 상의해서 후보를 몇 명 정해볼게요.', '새로운 복사기가 생길 때까지는 어떻게 해야 할 지 모르겠네요.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU3\n",
        "Llama2 파인튜닝 모델의 BLEU score 측정"
      ],
      "metadata": {
        "id": "0esq1iTV-L-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# english_sentences : 정답문장\n",
        "# translate_sentences : 번역문장\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "bleu_list = []\n",
        "for i in range(sample_num) :\n",
        "  correct = english_sentences[i].split()\n",
        "  translate = translate_sentences[i].split()\n",
        "\n",
        "  bleu_list.append(sentence_bleu([correct], translate, weights = (1, 0, 0, 0)))\n",
        "\n",
        "bleu_score = np.mean(bleu_list)\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyGGtFwJWj_i",
        "outputId": "460e022c-78af-458e-d791-b537371c0eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4460977814030677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련데이터에서 선택한 한국어 문장을 구글 번역기로 번역한 뒤, BLEU score를 측정"
      ],
      "metadata": {
        "id": "YPJbDbqk-T82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_translate = [\"Have you submitted all of the employee questionnaires requested last week?\",\n",
        "                    \"Sales may increase for now, but I think it will be negative in the long run.\",\n",
        "                    \"I am against it. If overtime suddenly increases, it will be difficult for employees.\",\n",
        "                    \"Why didn't you participate in the training held this morning?\",\n",
        "                    \"Good for you! I will change your room from room 1 to room 5 right now.\",\n",
        "                    \"I think phone calls and emails are enough.\",\n",
        "                    \"On the other hand, there are customers who have not pointed it out for a long time.\",\n",
        "                    \"What do you think should be changed first, office supplies or furniture?\",\n",
        "                    \"Yes, both dates are fine, so pick one that is convenient for you.\",\n",
        "                    \"Yes, the results will be better if multiple people have ideas.\",\n",
        "                    \"You can order, but it will likely take about 2 months to receive it.\",\n",
        "                    \"What's the biggest reason people cancel their subscription to our magazine?\",\n",
        "                    \"Can I report that this year's sales have doubled compared to last year?\",\n",
        "                    \"But it's too big of a space to just leave alone. What's the reason?\",\n",
        "                    \"But recently, many customers are always pointing out the same design.\",\n",
        "                    \"I am of the opposite opinion, but will it proceed without a meeting?\",\n",
        "                    \"I've already reviewed it several times, but you can do it again if you want.\",\n",
        "                    \"Probably next Tuesday. I'll check again.\",\n",
        "                    \"Yes, I will discuss with the staff and select a few candidates.\",\n",
        "                    \"I don't know what to do until I get a new copier.\"]\n",
        "\n",
        "# english_sentences : 정답문장\n",
        "# google_translate : 구글 번역기문장\n",
        "\n",
        "bleu_list = []\n",
        "for i in range(sample_num) :\n",
        "  correct = english_sentences[i].split()\n",
        "  translate = google_translate[i].split()\n",
        "\n",
        "  bleu_list.append(sentence_bleu([correct], translate, weights = (1, 0, 0, 0)))\n",
        "\n",
        "bleu_score = np.mean(bleu_list)\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlpgkB5Ogj7W",
        "outputId": "8054e3c7-dad3-41a0-d6b9-b71aea7bc52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3947945414541477\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM4csIKY3y1ZbSlg0RDo9qf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7dd2132503a4fc9b0e4c3fa94fecd06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7db01cd277b94160aef754289494575a",
              "IPY_MODEL_91b60ca6055148e8b894fcb66c5b54c0",
              "IPY_MODEL_c8b08b4c049144e295c9af350186fb21"
            ],
            "layout": "IPY_MODEL_9eccac0dc9e6480cb301259b95369c68"
          }
        },
        "7db01cd277b94160aef754289494575a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f91eb2dade4c1d86273e1084e73c9e",
            "placeholder": "​",
            "style": "IPY_MODEL_f495532cbd084f5f9eb79437b96405a4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "91b60ca6055148e8b894fcb66c5b54c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c27000ea89f44ac28d7a142dc4a32dde",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2d2832c14b048be9e9da83efadd77f9",
            "value": 14
          }
        },
        "c8b08b4c049144e295c9af350186fb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f2257da065149dbab8ffc5304957a12",
            "placeholder": "​",
            "style": "IPY_MODEL_7b71dded5960402f8f4fa343e1249d65",
            "value": " 14/14 [01:06&lt;00:00,  4.62s/it]"
          }
        },
        "9eccac0dc9e6480cb301259b95369c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f91eb2dade4c1d86273e1084e73c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f495532cbd084f5f9eb79437b96405a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c27000ea89f44ac28d7a142dc4a32dde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d2832c14b048be9e9da83efadd77f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f2257da065149dbab8ffc5304957a12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b71dded5960402f8f4fa343e1249d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}